{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment, I followed the NLP tutorial on HuggingFace to finetune a pre-trained masked language model on a subset of the *imdb* movie review dataset. The code was mostly from the [HuggingFace tutorial](https://huggingface.co/learn/nlp-course/chapter7/3?fw=pt). For learning purpose, I added my comments to explain the purpose of the code and some cells to check the content the nested data objects, which other learners may refer to if necessary.\n",
    "\n",
    "Overall, it was an interesting domain adaptation experiment. The pre-trained masked language model originally predicted [MASK] tokens with generic words. But after finetuning, the model learned to fill in the [MASK]s with words related to movies, since it got to see more movie reviews. For example, the input sequence was `This is a great [MASK]`.\n",
    "\n",
    "**Before finetuning:**\n",
    "```\n",
    "This is a great deal.\n",
    "This is a great success.\n",
    "This is a great adventure.\n",
    "This is a great idea.\n",
    "This is a great feat.\n",
    "```\n",
    "**After finetuning:**\n",
    "```\n",
    "this is a great film.\n",
    "this is a great movie.\n",
    "this is a great idea.\n",
    "this is a great one.\n",
    "this is a great story.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "\n",
    "# the checkpoint to be used for domain adaptation is \"distilbert-base-uncased\"\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> DistilBERT number of parameters: 67M'\n",
      "'>>> BERT number of parameters: 110M'\n"
     ]
    }
   ],
   "source": [
    "# DistilBERT has far fewer parameters than BERT\n",
    "distilbert_num_parameters = model.num_parameters() / 1_000_000\n",
    "print(f\"'>>> DistilBERT number of parameters: {round(distilbert_num_parameters)}M'\")\n",
    "print(f\"'>>> BERT number of parameters: 110M'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [MASK] prediction: Before domain adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's see how the pretrained model predicts the [MASK] token on this simple sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is a great [MASK].\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 30522])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# output of the model is logits for each possible next token, for each position in the sequence\n",
    "token_logits = model(**inputs).logits\n",
    "token_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 2023, 2003, 1037, 2307,  103, 1012,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenized inputs to the model\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note that the [MASK] token has been converted into the id \"103\" by the tokenizer\n",
    "tokenizer.mask_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 30522])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0]), tensor([5]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this function finds which token in the input sequence is the [MASK] token - that's the prediction we want to look at\n",
    "torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask_token_index = tensor([5])\n",
      "mask_token_logits.shape = torch.Size([1, 30522])\n"
     ]
    }
   ],
   "source": [
    "# Find the location of [MASK] and extract its logits\n",
    "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "print(f\"mask_token_index = {mask_token_index}\")\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "print(f\"mask_token_logits.shape = {mask_token_logits.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.return_types.topk(\n",
      "values=tensor([[7.0727, 6.6514, 6.6425, 6.2530, 5.8618]], grad_fn=<TopkBackward0>),\n",
      "indices=tensor([[3066, 3112, 6172, 2801, 8658]]))\n",
      "tensor([[3066, 3112, 6172, 2801, 8658]])\n",
      "[3066, 3112, 6172, 2801, 8658]\n"
     ]
    }
   ],
   "source": [
    "# pick the [MASK] candidates with the highest logits\n",
    "temp = torch.topk(mask_token_logits, 5, dim=1)\n",
    "print(temp)\n",
    "temp = temp.indices\n",
    "print(temp)\n",
    "temp = temp[0].tolist()\n",
    "print(temp)\n",
    "top_5_tokens = temp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('[MASK]', 'deal')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the top prediction to replace the [MASK] token in the sentence is \"deal\"\n",
    "tokenizer.mask_token, tokenizer.decode([top_5_tokens[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As observed in the HuggingFace tutorial, the top 5 predicted tokens to replace [MASK] all make sense, but are quite generic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'>>> This is a great deal.'\n",
      "'>>> This is a great success.'\n",
      "'>>> This is a great adventure.'\n",
      "'>>> This is a great idea.'\n",
      "'>>> This is a great feat.'\n"
     ]
    }
   ],
   "source": [
    "for token in top_5_tokens:\n",
    "    print(f\"'>>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [MASK] prediction: After domain adaptation with *imdb* movie reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will finetune this pre-trained DistilBERT on a small part of \"immdb\" dataset to see how the [MASK] predictions change\n",
    "from datasets import load_dataset\n",
    "imdb_dataset = load_dataset(\"imdb\")\n",
    "imdb_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has a \"label\" column for positivity rating, but we won't use it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = imdb_dataset[\"train\"].shuffle(seed=42).select(range(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 3\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Review: There is no relation at all between Fortier and Profiler but the fact that both are police series about violent crimes. Profiler looks crispy, Fortier looks classic. Profiler plots are quite simple. Fortier's plot are far more complicated... Fortier looks more like Prime Suspect, if we have to spot similarities... The main character is weak and weirdo, but have \"clairvoyance\". People like to compare, to judge, to evaluate. How about just enjoying? Funny thing too, people writing Fortier looks American but, on the other hand, arguing they prefer American series (!!!). Maybe it's the language, or the spirit, but I think this series is more English than American. By the way, the actors are really good and funny. The acting is not superficial at all...\n",
      ">>> Label: 1\n",
      "\n",
      ">>> Review: This movie is a great. The plot is very true to the book which is a classic written by Mark Twain. The movie starts of with a scene where Hank sings a song with a bunch of kids called \"when you stub your toe on the moon\" It reminds me of Sinatra's song High Hopes, it is fun and inspirational. The Music is great throughout and my favorite song is sung by the King, Hank (bing Crosby) and Sir \"Saggy\" Sagamore. OVerall a great family movie or even a great Date movie. This is a movie you can watch over and over again. The princess played by Rhonda Fleming is gorgeous. I love this movie!! If you liked Danny Kaye in the Court Jester then you will definitely like this movie.\n",
      ">>> Label: 1\n",
      "\n",
      ">>> Review: George P. Cosmatos' \"Rambo: First Blood Part II\" is pure wish-fulfillment. The United States clearly didn't win the war in Vietnam. They caused damage to this country beyond the imaginable and this movie continues the fairy story of the oh-so innocent soldiers. The only bad guys were the leaders of the nation, who made this war happen. The character of Rambo is perfect to notice this. He is extremely patriotic, bemoans that US-Americans didn't appreciate and celebrate the achievements of the single soldier, but has nothing but distrust for leading officers and politicians. Like every film that defends the war (e.g. \"We Were Soldiers\") also this one avoids the need to give a comprehensible reason for the engagement in South Asia. And for that matter also the reason for every single US-American soldier that was there. Instead, Rambo gets to take revenge for the wounds of a whole nation. It would have been better to work on how to deal with the memories, rather than suppressing them. \"Do we get to win this time?\" Yes, you do.\n",
      ">>> Label: 0\n"
     ]
    }
   ],
   "source": [
    "for row in sample:\n",
    "    print(f\"\\n>>> Review: {row['text']}\")\n",
    "    print(f\">>> Label: {row['label']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Review: If you've seen the classic Roger Corman version starring Vincent Price it's hard to put it out of your head, but you probably should do because this one is totally different. Subtlety has been abandoned in favour of gross-out horror - nudity, gore and all-round unpleasantness. OK it's ridiculous, trashy, sensationalised and historically dubious (did any members of the Inquisition really wear horn-rimmed glasses?), but despite all this it is strangely compelling. I literally couldn't tear myself away from the screen until the end of the movie. If there's a bigger compliment you can pay to a film I don't know what it is.\n",
      ">>> Label: -1\n",
      "\n",
      ">>> Review: For me, this was the most moving film of the decade. Samira Makhmalbaf shows pure bravery and vision in the making. She has an intelligence and gift for speaking to the people, regardless of their nationality or beliefs. I am inspired and touched by her humanity and can only hope that she has touched many people the same way. Her message in this film is strong, simple and pure. The human soul can survive the most unheard of cruelties and repression, yet still have the capability to hope and dream even the biggest dreams. Under the most incredible circumstances, the most unexpected people rise up to be heroes. This young girl who has recently regained her voice, yet is still afraid to use her new found freedom, is our hero. She daydreams of becoming president of war torn Afghanistan, the only vision of power that she can imagine that could truly change her current situation. We catch a glimpse of her spirit while witnessing her hardships. In the end, we are left with hope, hope that when her young voice does eventually speak out, it speaks loud and clear for all to hear- sounding a message that transcends borders, nationality and religion. The true epitome of the phoenix rising from the ashes. Hats off to the simple tale of the complex truth.\n",
      ">>> Label: -1\n",
      "\n",
      ">>> Review: There really isn't much to say about this \"film\". It has the odd smile or chuckle moment, but on the whole it's bland, predictable and generally pretty dull.<br /><br />The only reason I gave it three out of ten was for the annoyingly catchy jingle (which I hope I will forget soon....please God!). Otherwise its junk. Or mostly junk, interspersed with adverts for Smirnoff Ice.<br /><br />The lead characters give OK performances, but they really don't have anything much to work with.<br /><br />Best advice: Avoid it like a dentist's appointment. Or better yet, make a dentist's appointment instead of watching it.\n",
      ">>> Label: -1\n"
     ]
    }
   ],
   "source": [
    "sample_unsupervised = imdb_dataset[\"unsupervised\"].shuffle(seed=42).select(range(3))\n",
    "\n",
    "for row in sample_unsupervised:\n",
    "    print(f\"\\n>>> Review: {row['text']}\")\n",
    "    print(f\">>> Label: {row['label']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We keep \"word_ids\" column (which maps each token to which word it comes from in the original input sentence) during tokenizing, because we want to be able to do \"whole word masking\" instead of individual token masking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    result = tokenizer(examples[\"text\"])\n",
    "    if tokenizer.is_fast:\n",
    "        result[\"word_ids\"] = [result.word_ids(i) for i in range(len(result[\"input_ids\"]))]\n",
    "    return result\n",
    "\n",
    "tokenized_datasets = imdb_dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=[\"text\", \"label\"]\n",
    ")\n",
    "\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the tokenized sequences are of different lengths. So we concatenate all sequences and divide into equal chunks of length 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101,\n",
       "   1045,\n",
       "   12524,\n",
       "   1045,\n",
       "   2572,\n",
       "   8025,\n",
       "   1011,\n",
       "   3756,\n",
       "   2013,\n",
       "   2026,\n",
       "   2678,\n",
       "   3573,\n",
       "   2138,\n",
       "   1997,\n",
       "   2035,\n",
       "   1996,\n",
       "   6704,\n",
       "   2008,\n",
       "   5129,\n",
       "   2009,\n",
       "   2043,\n",
       "   2009,\n",
       "   2001,\n",
       "   2034,\n",
       "   2207,\n",
       "   1999,\n",
       "   3476,\n",
       "   1012,\n",
       "   1045,\n",
       "   2036,\n",
       "   2657,\n",
       "   2008,\n",
       "   2012,\n",
       "   2034,\n",
       "   2009,\n",
       "   2001,\n",
       "   8243,\n",
       "   2011,\n",
       "   1057,\n",
       "   1012,\n",
       "   1055,\n",
       "   1012,\n",
       "   8205,\n",
       "   2065,\n",
       "   2009,\n",
       "   2412,\n",
       "   2699,\n",
       "   2000,\n",
       "   4607,\n",
       "   2023,\n",
       "   2406,\n",
       "   1010,\n",
       "   3568,\n",
       "   2108,\n",
       "   1037,\n",
       "   5470,\n",
       "   1997,\n",
       "   3152,\n",
       "   2641,\n",
       "   1000,\n",
       "   6801,\n",
       "   1000,\n",
       "   1045,\n",
       "   2428,\n",
       "   2018,\n",
       "   2000,\n",
       "   2156,\n",
       "   2023,\n",
       "   2005,\n",
       "   2870,\n",
       "   1012,\n",
       "   1026,\n",
       "   7987,\n",
       "   1013,\n",
       "   1028,\n",
       "   1026,\n",
       "   7987,\n",
       "   1013,\n",
       "   1028,\n",
       "   1996,\n",
       "   5436,\n",
       "   2003,\n",
       "   8857,\n",
       "   2105,\n",
       "   1037,\n",
       "   2402,\n",
       "   4467,\n",
       "   3689,\n",
       "   3076,\n",
       "   2315,\n",
       "   14229,\n",
       "   2040,\n",
       "   4122,\n",
       "   2000,\n",
       "   4553,\n",
       "   2673,\n",
       "   2016,\n",
       "   2064,\n",
       "   2055,\n",
       "   2166,\n",
       "   1012,\n",
       "   1999,\n",
       "   3327,\n",
       "   2016,\n",
       "   4122,\n",
       "   2000,\n",
       "   3579,\n",
       "   2014,\n",
       "   3086,\n",
       "   2015,\n",
       "   2000,\n",
       "   2437,\n",
       "   2070,\n",
       "   4066,\n",
       "   1997,\n",
       "   4516,\n",
       "   2006,\n",
       "   2054,\n",
       "   1996,\n",
       "   2779,\n",
       "   25430,\n",
       "   14728,\n",
       "   2245,\n",
       "   2055,\n",
       "   3056,\n",
       "   2576,\n",
       "   3314,\n",
       "   2107,\n",
       "   2004,\n",
       "   1996,\n",
       "   5148,\n",
       "   2162,\n",
       "   1998,\n",
       "   2679,\n",
       "   3314,\n",
       "   1999,\n",
       "   1996,\n",
       "   2142,\n",
       "   2163,\n",
       "   1012,\n",
       "   1999,\n",
       "   2090,\n",
       "   4851,\n",
       "   8801,\n",
       "   1998,\n",
       "   6623,\n",
       "   7939,\n",
       "   4697,\n",
       "   3619,\n",
       "   1997,\n",
       "   8947,\n",
       "   2055,\n",
       "   2037,\n",
       "   10740,\n",
       "   2006,\n",
       "   4331,\n",
       "   1010,\n",
       "   2016,\n",
       "   2038,\n",
       "   3348,\n",
       "   2007,\n",
       "   2014,\n",
       "   3689,\n",
       "   3836,\n",
       "   1010,\n",
       "   19846,\n",
       "   1010,\n",
       "   1998,\n",
       "   2496,\n",
       "   2273,\n",
       "   1012,\n",
       "   1026,\n",
       "   7987,\n",
       "   1013,\n",
       "   1028,\n",
       "   1026,\n",
       "   7987,\n",
       "   1013,\n",
       "   1028,\n",
       "   2054,\n",
       "   8563,\n",
       "   2033,\n",
       "   2055,\n",
       "   1045,\n",
       "   2572,\n",
       "   8025,\n",
       "   1011,\n",
       "   3756,\n",
       "   2003,\n",
       "   2008,\n",
       "   2871,\n",
       "   2086,\n",
       "   3283,\n",
       "   1010,\n",
       "   2023,\n",
       "   2001,\n",
       "   2641,\n",
       "   26932,\n",
       "   1012,\n",
       "   2428,\n",
       "   1010,\n",
       "   1996,\n",
       "   3348,\n",
       "   1998,\n",
       "   16371,\n",
       "   25469,\n",
       "   5019,\n",
       "   2024,\n",
       "   2261,\n",
       "   1998,\n",
       "   2521,\n",
       "   2090,\n",
       "   1010,\n",
       "   2130,\n",
       "   2059,\n",
       "   2009,\n",
       "   1005,\n",
       "   1055,\n",
       "   2025,\n",
       "   2915,\n",
       "   2066,\n",
       "   2070,\n",
       "   10036,\n",
       "   2135,\n",
       "   2081,\n",
       "   22555,\n",
       "   2080,\n",
       "   1012,\n",
       "   2096,\n",
       "   2026,\n",
       "   2406,\n",
       "   3549,\n",
       "   2568,\n",
       "   2424,\n",
       "   2009,\n",
       "   16880,\n",
       "   1010,\n",
       "   1999,\n",
       "   4507,\n",
       "   3348,\n",
       "   1998,\n",
       "   16371,\n",
       "   25469,\n",
       "   2024,\n",
       "   1037,\n",
       "   2350,\n",
       "   18785,\n",
       "   1999,\n",
       "   4467,\n",
       "   5988,\n",
       "   1012,\n",
       "   2130,\n",
       "   13749,\n",
       "   7849,\n",
       "   24544,\n",
       "   1010,\n",
       "   15835,\n",
       "   2037,\n",
       "   3437,\n",
       "   2000,\n",
       "   2204,\n",
       "   2214,\n",
       "   2879,\n",
       "   2198,\n",
       "   4811,\n",
       "   1010,\n",
       "   2018,\n",
       "   3348,\n",
       "   5019,\n",
       "   1999,\n",
       "   2010,\n",
       "   3152,\n",
       "   1012,\n",
       "   1026,\n",
       "   7987,\n",
       "   1013,\n",
       "   1028,\n",
       "   1026,\n",
       "   7987,\n",
       "   1013,\n",
       "   1028,\n",
       "   1045,\n",
       "   2079,\n",
       "   4012,\n",
       "   3549,\n",
       "   2094,\n",
       "   1996,\n",
       "   16587,\n",
       "   2005,\n",
       "   1996,\n",
       "   2755,\n",
       "   2008,\n",
       "   2151,\n",
       "   3348,\n",
       "   3491,\n",
       "   1999,\n",
       "   1996,\n",
       "   2143,\n",
       "   2003,\n",
       "   3491,\n",
       "   2005,\n",
       "   6018,\n",
       "   5682,\n",
       "   2738,\n",
       "   2084,\n",
       "   2074,\n",
       "   2000,\n",
       "   5213,\n",
       "   2111,\n",
       "   1998,\n",
       "   2191,\n",
       "   2769,\n",
       "   2000,\n",
       "   2022,\n",
       "   3491,\n",
       "   1999,\n",
       "   26932,\n",
       "   12370,\n",
       "   1999,\n",
       "   2637,\n",
       "   1012,\n",
       "   1045,\n",
       "   2572,\n",
       "   8025,\n",
       "   1011,\n",
       "   3756,\n",
       "   2003,\n",
       "   1037,\n",
       "   2204,\n",
       "   2143,\n",
       "   2005,\n",
       "   3087,\n",
       "   5782,\n",
       "   2000,\n",
       "   2817,\n",
       "   1996,\n",
       "   6240,\n",
       "   1998,\n",
       "   14629,\n",
       "   1006,\n",
       "   2053,\n",
       "   26136,\n",
       "   3832,\n",
       "   1007,\n",
       "   1997,\n",
       "   4467,\n",
       "   5988,\n",
       "   1012,\n",
       "   2021,\n",
       "   2428,\n",
       "   1010,\n",
       "   2023,\n",
       "   2143,\n",
       "   2987,\n",
       "   1005,\n",
       "   1056,\n",
       "   2031,\n",
       "   2172,\n",
       "   1997,\n",
       "   1037,\n",
       "   5436,\n",
       "   1012,\n",
       "   102],\n",
       "  [101,\n",
       "   1000,\n",
       "   1045,\n",
       "   2572,\n",
       "   8025,\n",
       "   1024,\n",
       "   3756,\n",
       "   1000,\n",
       "   2003,\n",
       "   1037,\n",
       "   15544,\n",
       "   19307,\n",
       "   1998,\n",
       "   3653,\n",
       "   6528,\n",
       "   20771,\n",
       "   19986,\n",
       "   8632,\n",
       "   1012,\n",
       "   2009,\n",
       "   2987,\n",
       "   1005,\n",
       "   1056,\n",
       "   3043,\n",
       "   2054,\n",
       "   2028,\n",
       "   1005,\n",
       "   1055,\n",
       "   2576,\n",
       "   5328,\n",
       "   2024,\n",
       "   2138,\n",
       "   2023,\n",
       "   2143,\n",
       "   2064,\n",
       "   6684,\n",
       "   2022,\n",
       "   2579,\n",
       "   5667,\n",
       "   2006,\n",
       "   2151,\n",
       "   2504,\n",
       "   1012,\n",
       "   2004,\n",
       "   2005,\n",
       "   1996,\n",
       "   4366,\n",
       "   2008,\n",
       "   19124,\n",
       "   3287,\n",
       "   16371,\n",
       "   25469,\n",
       "   2003,\n",
       "   2019,\n",
       "   6882,\n",
       "   13316,\n",
       "   1011,\n",
       "   2459,\n",
       "   1010,\n",
       "   2008,\n",
       "   3475,\n",
       "   1005,\n",
       "   1056,\n",
       "   2995,\n",
       "   1012,\n",
       "   1045,\n",
       "   1005,\n",
       "   2310,\n",
       "   2464,\n",
       "   1054,\n",
       "   1011,\n",
       "   6758,\n",
       "   3152,\n",
       "   2007,\n",
       "   3287,\n",
       "   16371,\n",
       "   25469,\n",
       "   1012,\n",
       "   4379,\n",
       "   1010,\n",
       "   2027,\n",
       "   2069,\n",
       "   3749,\n",
       "   2070,\n",
       "   25085,\n",
       "   5328,\n",
       "   1010,\n",
       "   2021,\n",
       "   2073,\n",
       "   2024,\n",
       "   1996,\n",
       "   1054,\n",
       "   1011,\n",
       "   6758,\n",
       "   3152,\n",
       "   2007,\n",
       "   21226,\n",
       "   24728,\n",
       "   22144,\n",
       "   2015,\n",
       "   1998,\n",
       "   20916,\n",
       "   4691,\n",
       "   6845,\n",
       "   2401,\n",
       "   1029,\n",
       "   7880,\n",
       "   1010,\n",
       "   2138,\n",
       "   2027,\n",
       "   2123,\n",
       "   1005,\n",
       "   1056,\n",
       "   4839,\n",
       "   1012,\n",
       "   1996,\n",
       "   2168,\n",
       "   3632,\n",
       "   2005,\n",
       "   2216,\n",
       "   10231,\n",
       "   7685,\n",
       "   5830,\n",
       "   3065,\n",
       "   1024,\n",
       "   8040,\n",
       "   7317,\n",
       "   5063,\n",
       "   2015,\n",
       "   11820,\n",
       "   1999,\n",
       "   1996,\n",
       "   9478,\n",
       "   2021,\n",
       "   2025,\n",
       "   1037,\n",
       "   17962,\n",
       "   21239,\n",
       "   1999,\n",
       "   4356,\n",
       "   1012,\n",
       "   1998,\n",
       "   2216,\n",
       "   3653,\n",
       "   6528,\n",
       "   20771,\n",
       "   10271,\n",
       "   5691,\n",
       "   2066,\n",
       "   1996,\n",
       "   2829,\n",
       "   16291,\n",
       "   1010,\n",
       "   1999,\n",
       "   2029,\n",
       "   2057,\n",
       "   1005,\n",
       "   2128,\n",
       "   5845,\n",
       "   2000,\n",
       "   1996,\n",
       "   2609,\n",
       "   1997,\n",
       "   6320,\n",
       "   25624,\n",
       "   1005,\n",
       "   1055,\n",
       "   17061,\n",
       "   3779,\n",
       "   1010,\n",
       "   2021,\n",
       "   2025,\n",
       "   1037,\n",
       "   7637,\n",
       "   1997,\n",
       "   5061,\n",
       "   5710,\n",
       "   2006,\n",
       "   9318,\n",
       "   7367,\n",
       "   5737,\n",
       "   19393,\n",
       "   1012,\n",
       "   2077,\n",
       "   6933,\n",
       "   1006,\n",
       "   2030,\n",
       "   20242,\n",
       "   1007,\n",
       "   1000,\n",
       "   3313,\n",
       "   1011,\n",
       "   3115,\n",
       "   1000,\n",
       "   1999,\n",
       "   5609,\n",
       "   1997,\n",
       "   16371,\n",
       "   25469,\n",
       "   1010,\n",
       "   1996,\n",
       "   10597,\n",
       "   27885,\n",
       "   5809,\n",
       "   2063,\n",
       "   2323,\n",
       "   2202,\n",
       "   2046,\n",
       "   4070,\n",
       "   2028,\n",
       "   14477,\n",
       "   6767,\n",
       "   8524,\n",
       "   6321,\n",
       "   5793,\n",
       "   28141,\n",
       "   4489,\n",
       "   2090,\n",
       "   2273,\n",
       "   1998,\n",
       "   2308,\n",
       "   1024,\n",
       "   2045,\n",
       "   2024,\n",
       "   2053,\n",
       "   8991,\n",
       "   18400,\n",
       "   2015,\n",
       "   2006,\n",
       "   4653,\n",
       "   2043,\n",
       "   19910,\n",
       "   3544,\n",
       "   15287,\n",
       "   1010,\n",
       "   1998,\n",
       "   1996,\n",
       "   2168,\n",
       "   3685,\n",
       "   2022,\n",
       "   2056,\n",
       "   2005,\n",
       "   1037,\n",
       "   2158,\n",
       "   1012,\n",
       "   1999,\n",
       "   2755,\n",
       "   1010,\n",
       "   2017,\n",
       "   3227,\n",
       "   2180,\n",
       "   1005,\n",
       "   1056,\n",
       "   2156,\n",
       "   2931,\n",
       "   8991,\n",
       "   18400,\n",
       "   2015,\n",
       "   1999,\n",
       "   2019,\n",
       "   2137,\n",
       "   2143,\n",
       "   1999,\n",
       "   2505,\n",
       "   2460,\n",
       "   1997,\n",
       "   22555,\n",
       "   2030,\n",
       "   13216,\n",
       "   14253,\n",
       "   2050,\n",
       "   1012,\n",
       "   2023,\n",
       "   6884,\n",
       "   3313,\n",
       "   1011,\n",
       "   3115,\n",
       "   2003,\n",
       "   2625,\n",
       "   1037,\n",
       "   3313,\n",
       "   3115,\n",
       "   2084,\n",
       "   2019,\n",
       "   4914,\n",
       "   2135,\n",
       "   2139,\n",
       "   24128,\n",
       "   3754,\n",
       "   2000,\n",
       "   2272,\n",
       "   2000,\n",
       "   3408,\n",
       "   20547,\n",
       "   2007,\n",
       "   1996,\n",
       "   19008,\n",
       "   1997,\n",
       "   2308,\n",
       "   1005,\n",
       "   1055,\n",
       "   4230,\n",
       "   1012,\n",
       "   102],\n",
       "  [101,\n",
       "   2065,\n",
       "   2069,\n",
       "   2000,\n",
       "   4468,\n",
       "   2437,\n",
       "   2023,\n",
       "   2828,\n",
       "   1997,\n",
       "   2143,\n",
       "   1999,\n",
       "   1996,\n",
       "   2925,\n",
       "   1012,\n",
       "   2023,\n",
       "   2143,\n",
       "   2003,\n",
       "   5875,\n",
       "   2004,\n",
       "   2019,\n",
       "   7551,\n",
       "   2021,\n",
       "   4136,\n",
       "   2053,\n",
       "   2522,\n",
       "   11461,\n",
       "   2466,\n",
       "   1012,\n",
       "   1026,\n",
       "   7987,\n",
       "   1013,\n",
       "   1028,\n",
       "   1026,\n",
       "   7987,\n",
       "   1013,\n",
       "   1028,\n",
       "   2028,\n",
       "   2453,\n",
       "   2514,\n",
       "   6819,\n",
       "   5339,\n",
       "   8918,\n",
       "   2005,\n",
       "   3564,\n",
       "   27046,\n",
       "   2009,\n",
       "   2138,\n",
       "   2009,\n",
       "   12817,\n",
       "   2006,\n",
       "   2061,\n",
       "   2116,\n",
       "   2590,\n",
       "   3314,\n",
       "   2021,\n",
       "   2009,\n",
       "   2515,\n",
       "   2061,\n",
       "   2302,\n",
       "   2151,\n",
       "   5860,\n",
       "   11795,\n",
       "   3085,\n",
       "   15793,\n",
       "   1012,\n",
       "   1996,\n",
       "   13972,\n",
       "   3310,\n",
       "   2185,\n",
       "   2007,\n",
       "   2053,\n",
       "   2047,\n",
       "   15251,\n",
       "   1006,\n",
       "   4983,\n",
       "   2028,\n",
       "   3310,\n",
       "   2039,\n",
       "   2007,\n",
       "   2028,\n",
       "   2096,\n",
       "   2028,\n",
       "   1005,\n",
       "   1055,\n",
       "   2568,\n",
       "   17677,\n",
       "   2015,\n",
       "   1010,\n",
       "   2004,\n",
       "   2009,\n",
       "   2097,\n",
       "   26597,\n",
       "   2079,\n",
       "   2076,\n",
       "   2023,\n",
       "   23100,\n",
       "   2143,\n",
       "   1007,\n",
       "   1012,\n",
       "   1026,\n",
       "   7987,\n",
       "   1013,\n",
       "   1028,\n",
       "   1026,\n",
       "   7987,\n",
       "   1013,\n",
       "   1028,\n",
       "   2028,\n",
       "   2453,\n",
       "   2488,\n",
       "   5247,\n",
       "   2028,\n",
       "   1005,\n",
       "   1055,\n",
       "   2051,\n",
       "   4582,\n",
       "   2041,\n",
       "   1037,\n",
       "   3332,\n",
       "   2012,\n",
       "   1037,\n",
       "   3392,\n",
       "   3652,\n",
       "   1012,\n",
       "   1026,\n",
       "   7987,\n",
       "   1013,\n",
       "   1028,\n",
       "   1026,\n",
       "   7987,\n",
       "   1013,\n",
       "   1028,\n",
       "   102]],\n",
       " 'attention_mask': [[1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1],\n",
       "  [1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1,\n",
       "   1]],\n",
       " 'word_ids': [[None,\n",
       "   0,\n",
       "   1,\n",
       "   2,\n",
       "   3,\n",
       "   4,\n",
       "   5,\n",
       "   6,\n",
       "   7,\n",
       "   8,\n",
       "   9,\n",
       "   10,\n",
       "   11,\n",
       "   12,\n",
       "   13,\n",
       "   14,\n",
       "   15,\n",
       "   16,\n",
       "   17,\n",
       "   18,\n",
       "   19,\n",
       "   20,\n",
       "   21,\n",
       "   22,\n",
       "   23,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31,\n",
       "   32,\n",
       "   33,\n",
       "   34,\n",
       "   35,\n",
       "   36,\n",
       "   37,\n",
       "   38,\n",
       "   39,\n",
       "   40,\n",
       "   41,\n",
       "   42,\n",
       "   43,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   47,\n",
       "   48,\n",
       "   49,\n",
       "   50,\n",
       "   51,\n",
       "   52,\n",
       "   53,\n",
       "   54,\n",
       "   55,\n",
       "   56,\n",
       "   57,\n",
       "   58,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63,\n",
       "   64,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   69,\n",
       "   70,\n",
       "   71,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   82,\n",
       "   83,\n",
       "   84,\n",
       "   85,\n",
       "   86,\n",
       "   87,\n",
       "   88,\n",
       "   89,\n",
       "   90,\n",
       "   91,\n",
       "   92,\n",
       "   93,\n",
       "   94,\n",
       "   95,\n",
       "   96,\n",
       "   97,\n",
       "   98,\n",
       "   99,\n",
       "   100,\n",
       "   101,\n",
       "   102,\n",
       "   103,\n",
       "   104,\n",
       "   105,\n",
       "   106,\n",
       "   107,\n",
       "   107,\n",
       "   108,\n",
       "   109,\n",
       "   110,\n",
       "   111,\n",
       "   112,\n",
       "   113,\n",
       "   114,\n",
       "   115,\n",
       "   116,\n",
       "   117,\n",
       "   118,\n",
       "   118,\n",
       "   119,\n",
       "   120,\n",
       "   121,\n",
       "   122,\n",
       "   123,\n",
       "   124,\n",
       "   125,\n",
       "   126,\n",
       "   127,\n",
       "   128,\n",
       "   129,\n",
       "   130,\n",
       "   131,\n",
       "   132,\n",
       "   133,\n",
       "   134,\n",
       "   135,\n",
       "   136,\n",
       "   137,\n",
       "   138,\n",
       "   139,\n",
       "   140,\n",
       "   141,\n",
       "   142,\n",
       "   143,\n",
       "   143,\n",
       "   143,\n",
       "   144,\n",
       "   145,\n",
       "   146,\n",
       "   147,\n",
       "   148,\n",
       "   149,\n",
       "   150,\n",
       "   151,\n",
       "   152,\n",
       "   153,\n",
       "   154,\n",
       "   155,\n",
       "   156,\n",
       "   157,\n",
       "   158,\n",
       "   159,\n",
       "   160,\n",
       "   161,\n",
       "   162,\n",
       "   163,\n",
       "   164,\n",
       "   165,\n",
       "   166,\n",
       "   167,\n",
       "   168,\n",
       "   169,\n",
       "   170,\n",
       "   171,\n",
       "   172,\n",
       "   173,\n",
       "   174,\n",
       "   175,\n",
       "   176,\n",
       "   177,\n",
       "   178,\n",
       "   179,\n",
       "   180,\n",
       "   181,\n",
       "   182,\n",
       "   183,\n",
       "   184,\n",
       "   185,\n",
       "   186,\n",
       "   187,\n",
       "   188,\n",
       "   189,\n",
       "   190,\n",
       "   191,\n",
       "   192,\n",
       "   193,\n",
       "   194,\n",
       "   195,\n",
       "   196,\n",
       "   197,\n",
       "   198,\n",
       "   199,\n",
       "   199,\n",
       "   200,\n",
       "   201,\n",
       "   202,\n",
       "   203,\n",
       "   204,\n",
       "   205,\n",
       "   206,\n",
       "   207,\n",
       "   208,\n",
       "   209,\n",
       "   210,\n",
       "   211,\n",
       "   212,\n",
       "   213,\n",
       "   214,\n",
       "   215,\n",
       "   216,\n",
       "   216,\n",
       "   217,\n",
       "   218,\n",
       "   218,\n",
       "   219,\n",
       "   220,\n",
       "   221,\n",
       "   222,\n",
       "   222,\n",
       "   223,\n",
       "   224,\n",
       "   225,\n",
       "   226,\n",
       "   227,\n",
       "   228,\n",
       "   229,\n",
       "   230,\n",
       "   231,\n",
       "   232,\n",
       "   232,\n",
       "   233,\n",
       "   234,\n",
       "   235,\n",
       "   236,\n",
       "   237,\n",
       "   238,\n",
       "   239,\n",
       "   240,\n",
       "   241,\n",
       "   242,\n",
       "   242,\n",
       "   243,\n",
       "   244,\n",
       "   245,\n",
       "   246,\n",
       "   247,\n",
       "   248,\n",
       "   249,\n",
       "   250,\n",
       "   251,\n",
       "   252,\n",
       "   253,\n",
       "   254,\n",
       "   255,\n",
       "   256,\n",
       "   257,\n",
       "   258,\n",
       "   259,\n",
       "   260,\n",
       "   261,\n",
       "   262,\n",
       "   263,\n",
       "   264,\n",
       "   265,\n",
       "   266,\n",
       "   267,\n",
       "   268,\n",
       "   269,\n",
       "   270,\n",
       "   271,\n",
       "   272,\n",
       "   272,\n",
       "   272,\n",
       "   273,\n",
       "   274,\n",
       "   275,\n",
       "   276,\n",
       "   277,\n",
       "   278,\n",
       "   279,\n",
       "   280,\n",
       "   281,\n",
       "   282,\n",
       "   283,\n",
       "   284,\n",
       "   285,\n",
       "   286,\n",
       "   287,\n",
       "   288,\n",
       "   289,\n",
       "   290,\n",
       "   291,\n",
       "   292,\n",
       "   293,\n",
       "   294,\n",
       "   295,\n",
       "   296,\n",
       "   297,\n",
       "   298,\n",
       "   299,\n",
       "   300,\n",
       "   301,\n",
       "   302,\n",
       "   303,\n",
       "   304,\n",
       "   305,\n",
       "   306,\n",
       "   307,\n",
       "   308,\n",
       "   309,\n",
       "   310,\n",
       "   311,\n",
       "   312,\n",
       "   313,\n",
       "   314,\n",
       "   315,\n",
       "   316,\n",
       "   317,\n",
       "   318,\n",
       "   319,\n",
       "   320,\n",
       "   321,\n",
       "   322,\n",
       "   323,\n",
       "   324,\n",
       "   325,\n",
       "   326,\n",
       "   327,\n",
       "   328,\n",
       "   329,\n",
       "   330,\n",
       "   331,\n",
       "   332,\n",
       "   333,\n",
       "   334,\n",
       "   335,\n",
       "   336,\n",
       "   337,\n",
       "   338,\n",
       "   339,\n",
       "   340,\n",
       "   341,\n",
       "   342,\n",
       "   343,\n",
       "   344,\n",
       "   345,\n",
       "   346,\n",
       "   347,\n",
       "   348,\n",
       "   None],\n",
       "  [None,\n",
       "   0,\n",
       "   1,\n",
       "   2,\n",
       "   3,\n",
       "   4,\n",
       "   5,\n",
       "   6,\n",
       "   7,\n",
       "   8,\n",
       "   9,\n",
       "   9,\n",
       "   10,\n",
       "   11,\n",
       "   11,\n",
       "   11,\n",
       "   12,\n",
       "   13,\n",
       "   14,\n",
       "   15,\n",
       "   16,\n",
       "   17,\n",
       "   18,\n",
       "   19,\n",
       "   20,\n",
       "   21,\n",
       "   22,\n",
       "   23,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31,\n",
       "   32,\n",
       "   33,\n",
       "   34,\n",
       "   35,\n",
       "   36,\n",
       "   37,\n",
       "   38,\n",
       "   39,\n",
       "   40,\n",
       "   41,\n",
       "   42,\n",
       "   43,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   46,\n",
       "   47,\n",
       "   48,\n",
       "   49,\n",
       "   50,\n",
       "   51,\n",
       "   52,\n",
       "   53,\n",
       "   54,\n",
       "   55,\n",
       "   56,\n",
       "   57,\n",
       "   58,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63,\n",
       "   64,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   69,\n",
       "   70,\n",
       "   70,\n",
       "   71,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   82,\n",
       "   83,\n",
       "   84,\n",
       "   85,\n",
       "   86,\n",
       "   87,\n",
       "   88,\n",
       "   89,\n",
       "   90,\n",
       "   91,\n",
       "   91,\n",
       "   91,\n",
       "   92,\n",
       "   93,\n",
       "   93,\n",
       "   94,\n",
       "   94,\n",
       "   95,\n",
       "   96,\n",
       "   97,\n",
       "   98,\n",
       "   99,\n",
       "   100,\n",
       "   101,\n",
       "   102,\n",
       "   103,\n",
       "   104,\n",
       "   105,\n",
       "   106,\n",
       "   107,\n",
       "   108,\n",
       "   109,\n",
       "   110,\n",
       "   110,\n",
       "   111,\n",
       "   112,\n",
       "   113,\n",
       "   114,\n",
       "   114,\n",
       "   114,\n",
       "   114,\n",
       "   115,\n",
       "   116,\n",
       "   117,\n",
       "   118,\n",
       "   119,\n",
       "   120,\n",
       "   121,\n",
       "   122,\n",
       "   122,\n",
       "   123,\n",
       "   124,\n",
       "   125,\n",
       "   126,\n",
       "   127,\n",
       "   128,\n",
       "   128,\n",
       "   128,\n",
       "   129,\n",
       "   130,\n",
       "   131,\n",
       "   132,\n",
       "   133,\n",
       "   134,\n",
       "   135,\n",
       "   136,\n",
       "   137,\n",
       "   138,\n",
       "   139,\n",
       "   140,\n",
       "   141,\n",
       "   142,\n",
       "   143,\n",
       "   144,\n",
       "   145,\n",
       "   146,\n",
       "   147,\n",
       "   148,\n",
       "   149,\n",
       "   150,\n",
       "   151,\n",
       "   152,\n",
       "   153,\n",
       "   154,\n",
       "   155,\n",
       "   156,\n",
       "   157,\n",
       "   158,\n",
       "   159,\n",
       "   160,\n",
       "   161,\n",
       "   162,\n",
       "   162,\n",
       "   162,\n",
       "   163,\n",
       "   164,\n",
       "   165,\n",
       "   166,\n",
       "   167,\n",
       "   168,\n",
       "   169,\n",
       "   170,\n",
       "   171,\n",
       "   172,\n",
       "   173,\n",
       "   174,\n",
       "   175,\n",
       "   176,\n",
       "   177,\n",
       "   178,\n",
       "   178,\n",
       "   179,\n",
       "   180,\n",
       "   181,\n",
       "   182,\n",
       "   182,\n",
       "   182,\n",
       "   183,\n",
       "   184,\n",
       "   185,\n",
       "   186,\n",
       "   187,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   188,\n",
       "   189,\n",
       "   190,\n",
       "   191,\n",
       "   192,\n",
       "   193,\n",
       "   194,\n",
       "   195,\n",
       "   196,\n",
       "   197,\n",
       "   198,\n",
       "   199,\n",
       "   200,\n",
       "   200,\n",
       "   200,\n",
       "   201,\n",
       "   202,\n",
       "   203,\n",
       "   204,\n",
       "   205,\n",
       "   206,\n",
       "   207,\n",
       "   208,\n",
       "   209,\n",
       "   210,\n",
       "   211,\n",
       "   212,\n",
       "   213,\n",
       "   214,\n",
       "   215,\n",
       "   216,\n",
       "   217,\n",
       "   218,\n",
       "   219,\n",
       "   220,\n",
       "   221,\n",
       "   222,\n",
       "   223,\n",
       "   224,\n",
       "   225,\n",
       "   226,\n",
       "   227,\n",
       "   228,\n",
       "   228,\n",
       "   228,\n",
       "   229,\n",
       "   230,\n",
       "   231,\n",
       "   232,\n",
       "   233,\n",
       "   234,\n",
       "   235,\n",
       "   236,\n",
       "   237,\n",
       "   238,\n",
       "   239,\n",
       "   240,\n",
       "   240,\n",
       "   241,\n",
       "   242,\n",
       "   243,\n",
       "   244,\n",
       "   245,\n",
       "   246,\n",
       "   247,\n",
       "   248,\n",
       "   249,\n",
       "   250,\n",
       "   251,\n",
       "   252,\n",
       "   253,\n",
       "   254,\n",
       "   254,\n",
       "   255,\n",
       "   255,\n",
       "   256,\n",
       "   257,\n",
       "   258,\n",
       "   259,\n",
       "   260,\n",
       "   261,\n",
       "   262,\n",
       "   263,\n",
       "   264,\n",
       "   265,\n",
       "   266,\n",
       "   267,\n",
       "   268,\n",
       "   269,\n",
       "   270,\n",
       "   None],\n",
       "  [None,\n",
       "   0,\n",
       "   1,\n",
       "   2,\n",
       "   3,\n",
       "   4,\n",
       "   5,\n",
       "   6,\n",
       "   7,\n",
       "   8,\n",
       "   9,\n",
       "   10,\n",
       "   11,\n",
       "   12,\n",
       "   13,\n",
       "   14,\n",
       "   15,\n",
       "   16,\n",
       "   17,\n",
       "   18,\n",
       "   19,\n",
       "   20,\n",
       "   21,\n",
       "   22,\n",
       "   23,\n",
       "   23,\n",
       "   24,\n",
       "   25,\n",
       "   26,\n",
       "   27,\n",
       "   28,\n",
       "   29,\n",
       "   30,\n",
       "   31,\n",
       "   32,\n",
       "   33,\n",
       "   34,\n",
       "   35,\n",
       "   36,\n",
       "   37,\n",
       "   37,\n",
       "   37,\n",
       "   38,\n",
       "   39,\n",
       "   40,\n",
       "   41,\n",
       "   42,\n",
       "   43,\n",
       "   44,\n",
       "   45,\n",
       "   46,\n",
       "   47,\n",
       "   48,\n",
       "   49,\n",
       "   50,\n",
       "   51,\n",
       "   52,\n",
       "   53,\n",
       "   54,\n",
       "   55,\n",
       "   56,\n",
       "   56,\n",
       "   56,\n",
       "   57,\n",
       "   58,\n",
       "   59,\n",
       "   60,\n",
       "   61,\n",
       "   62,\n",
       "   63,\n",
       "   64,\n",
       "   65,\n",
       "   66,\n",
       "   67,\n",
       "   68,\n",
       "   69,\n",
       "   70,\n",
       "   71,\n",
       "   72,\n",
       "   73,\n",
       "   74,\n",
       "   75,\n",
       "   76,\n",
       "   77,\n",
       "   78,\n",
       "   79,\n",
       "   79,\n",
       "   80,\n",
       "   81,\n",
       "   82,\n",
       "   83,\n",
       "   84,\n",
       "   85,\n",
       "   86,\n",
       "   87,\n",
       "   88,\n",
       "   89,\n",
       "   90,\n",
       "   91,\n",
       "   92,\n",
       "   93,\n",
       "   94,\n",
       "   95,\n",
       "   96,\n",
       "   97,\n",
       "   98,\n",
       "   99,\n",
       "   100,\n",
       "   101,\n",
       "   102,\n",
       "   103,\n",
       "   104,\n",
       "   105,\n",
       "   106,\n",
       "   107,\n",
       "   108,\n",
       "   109,\n",
       "   110,\n",
       "   111,\n",
       "   112,\n",
       "   113,\n",
       "   114,\n",
       "   115,\n",
       "   116,\n",
       "   117,\n",
       "   118,\n",
       "   119,\n",
       "   120,\n",
       "   121,\n",
       "   122,\n",
       "   123,\n",
       "   124,\n",
       "   None]]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"train\"][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Review 0 length: 363\n",
      ">>> Review 1 length: 304\n",
      ">>> Review 2 length: 133\n"
     ]
    }
   ],
   "source": [
    "# checking the lengths of different tokenized sequences\n",
    "tokenized_samples = tokenized_datasets[\"train\"][:3]\n",
    "\n",
    "for idx, sample in enumerate(tokenized_samples[\"input_ids\"]):\n",
    "    print(f\">>> Review {idx} length: {len(sample)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 1045,\n",
       " 12524,\n",
       " 1045,\n",
       " 2572,\n",
       " 8025,\n",
       " 1011,\n",
       " 3756,\n",
       " 2013,\n",
       " 2026,\n",
       " 2678,\n",
       " 3573,\n",
       " 2138,\n",
       " 1997,\n",
       " 2035,\n",
       " 1996,\n",
       " 6704,\n",
       " 2008,\n",
       " 5129,\n",
       " 2009,\n",
       " 2043,\n",
       " 2009,\n",
       " 2001,\n",
       " 2034,\n",
       " 2207,\n",
       " 1999,\n",
       " 3476,\n",
       " 1012,\n",
       " 1045,\n",
       " 2036,\n",
       " 2657,\n",
       " 2008,\n",
       " 2012,\n",
       " 2034,\n",
       " 2009,\n",
       " 2001,\n",
       " 8243,\n",
       " 2011,\n",
       " 1057,\n",
       " 1012,\n",
       " 1055,\n",
       " 1012,\n",
       " 8205,\n",
       " 2065,\n",
       " 2009,\n",
       " 2412,\n",
       " 2699,\n",
       " 2000,\n",
       " 4607,\n",
       " 2023,\n",
       " 2406,\n",
       " 1010,\n",
       " 3568,\n",
       " 2108,\n",
       " 1037,\n",
       " 5470,\n",
       " 1997,\n",
       " 3152,\n",
       " 2641,\n",
       " 1000,\n",
       " 6801,\n",
       " 1000,\n",
       " 1045,\n",
       " 2428,\n",
       " 2018,\n",
       " 2000,\n",
       " 2156,\n",
       " 2023,\n",
       " 2005,\n",
       " 2870,\n",
       " 1012,\n",
       " 1026,\n",
       " 7987,\n",
       " 1013,\n",
       " 1028,\n",
       " 1026,\n",
       " 7987,\n",
       " 1013,\n",
       " 1028,\n",
       " 1996,\n",
       " 5436,\n",
       " 2003,\n",
       " 8857,\n",
       " 2105,\n",
       " 1037,\n",
       " 2402,\n",
       " 4467,\n",
       " 3689,\n",
       " 3076,\n",
       " 2315,\n",
       " 14229,\n",
       " 2040,\n",
       " 4122,\n",
       " 2000,\n",
       " 4553,\n",
       " 2673,\n",
       " 2016,\n",
       " 2064,\n",
       " 2055,\n",
       " 2166,\n",
       " 1012,\n",
       " 1999,\n",
       " 3327,\n",
       " 2016,\n",
       " 4122,\n",
       " 2000,\n",
       " 3579,\n",
       " 2014,\n",
       " 3086,\n",
       " 2015,\n",
       " 2000,\n",
       " 2437,\n",
       " 2070,\n",
       " 4066,\n",
       " 1997,\n",
       " 4516,\n",
       " 2006,\n",
       " 2054,\n",
       " 1996,\n",
       " 2779,\n",
       " 25430,\n",
       " 14728,\n",
       " 2245,\n",
       " 2055,\n",
       " 3056,\n",
       " 2576,\n",
       " 3314,\n",
       " 2107,\n",
       " 2004,\n",
       " 1996,\n",
       " 5148,\n",
       " 2162,\n",
       " 1998,\n",
       " 2679,\n",
       " 3314,\n",
       " 1999,\n",
       " 1996,\n",
       " 2142,\n",
       " 2163,\n",
       " 1012,\n",
       " 1999,\n",
       " 2090,\n",
       " 4851,\n",
       " 8801,\n",
       " 1998,\n",
       " 6623,\n",
       " 7939,\n",
       " 4697,\n",
       " 3619,\n",
       " 1997,\n",
       " 8947,\n",
       " 2055,\n",
       " 2037,\n",
       " 10740,\n",
       " 2006,\n",
       " 4331,\n",
       " 1010,\n",
       " 2016,\n",
       " 2038,\n",
       " 3348,\n",
       " 2007,\n",
       " 2014,\n",
       " 3689,\n",
       " 3836,\n",
       " 1010,\n",
       " 19846,\n",
       " 1010,\n",
       " 1998,\n",
       " 2496,\n",
       " 2273,\n",
       " 1012,\n",
       " 1026,\n",
       " 7987,\n",
       " 1013,\n",
       " 1028,\n",
       " 1026,\n",
       " 7987,\n",
       " 1013,\n",
       " 1028,\n",
       " 2054,\n",
       " 8563,\n",
       " 2033,\n",
       " 2055,\n",
       " 1045,\n",
       " 2572,\n",
       " 8025,\n",
       " 1011,\n",
       " 3756,\n",
       " 2003,\n",
       " 2008,\n",
       " 2871,\n",
       " 2086,\n",
       " 3283,\n",
       " 1010,\n",
       " 2023,\n",
       " 2001,\n",
       " 2641,\n",
       " 26932,\n",
       " 1012,\n",
       " 2428,\n",
       " 1010,\n",
       " 1996,\n",
       " 3348,\n",
       " 1998,\n",
       " 16371,\n",
       " 25469,\n",
       " 5019,\n",
       " 2024,\n",
       " 2261,\n",
       " 1998,\n",
       " 2521,\n",
       " 2090,\n",
       " 1010,\n",
       " 2130,\n",
       " 2059,\n",
       " 2009,\n",
       " 1005,\n",
       " 1055,\n",
       " 2025,\n",
       " 2915,\n",
       " 2066,\n",
       " 2070,\n",
       " 10036,\n",
       " 2135,\n",
       " 2081,\n",
       " 22555,\n",
       " 2080,\n",
       " 1012,\n",
       " 2096,\n",
       " 2026,\n",
       " 2406,\n",
       " 3549,\n",
       " 2568,\n",
       " 2424,\n",
       " 2009,\n",
       " 16880,\n",
       " 1010,\n",
       " 1999,\n",
       " 4507,\n",
       " 3348,\n",
       " 1998,\n",
       " 16371,\n",
       " 25469,\n",
       " 2024,\n",
       " 1037,\n",
       " 2350,\n",
       " 18785,\n",
       " 1999,\n",
       " 4467,\n",
       " 5988,\n",
       " 1012,\n",
       " 2130,\n",
       " 13749,\n",
       " 7849,\n",
       " 24544,\n",
       " 1010,\n",
       " 15835,\n",
       " 2037,\n",
       " 3437,\n",
       " 2000,\n",
       " 2204,\n",
       " 2214,\n",
       " 2879,\n",
       " 2198,\n",
       " 4811,\n",
       " 1010,\n",
       " 2018,\n",
       " 3348,\n",
       " 5019,\n",
       " 1999,\n",
       " 2010,\n",
       " 3152,\n",
       " 1012,\n",
       " 1026,\n",
       " 7987,\n",
       " 1013,\n",
       " 1028,\n",
       " 1026,\n",
       " 7987,\n",
       " 1013,\n",
       " 1028,\n",
       " 1045,\n",
       " 2079,\n",
       " 4012,\n",
       " 3549,\n",
       " 2094,\n",
       " 1996,\n",
       " 16587,\n",
       " 2005,\n",
       " 1996,\n",
       " 2755,\n",
       " 2008,\n",
       " 2151,\n",
       " 3348,\n",
       " 3491,\n",
       " 1999,\n",
       " 1996,\n",
       " 2143,\n",
       " 2003,\n",
       " 3491,\n",
       " 2005,\n",
       " 6018,\n",
       " 5682,\n",
       " 2738,\n",
       " 2084,\n",
       " 2074,\n",
       " 2000,\n",
       " 5213,\n",
       " 2111,\n",
       " 1998,\n",
       " 2191,\n",
       " 2769,\n",
       " 2000,\n",
       " 2022,\n",
       " 3491,\n",
       " 1999,\n",
       " 26932,\n",
       " 12370,\n",
       " 1999,\n",
       " 2637,\n",
       " 1012,\n",
       " 1045,\n",
       " 2572,\n",
       " 8025,\n",
       " 1011,\n",
       " 3756,\n",
       " 2003,\n",
       " 1037,\n",
       " 2204,\n",
       " 2143,\n",
       " 2005,\n",
       " 3087,\n",
       " 5782,\n",
       " 2000,\n",
       " 2817,\n",
       " 1996,\n",
       " 6240,\n",
       " 1998,\n",
       " 14629,\n",
       " 1006,\n",
       " 2053,\n",
       " 26136,\n",
       " 3832,\n",
       " 1007,\n",
       " 1997,\n",
       " 4467,\n",
       " 5988,\n",
       " 1012,\n",
       " 2021,\n",
       " 2428,\n",
       " 1010,\n",
       " 2023,\n",
       " 2143,\n",
       " 2987,\n",
       " 1005,\n",
       " 1056,\n",
       " 2031,\n",
       " 2172,\n",
       " 1997,\n",
       " 1037,\n",
       " 5436,\n",
       " 1012,\n",
       " 102,\n",
       " 101,\n",
       " 1000,\n",
       " 1045,\n",
       " 2572,\n",
       " 8025,\n",
       " 1024,\n",
       " 3756,\n",
       " 1000,\n",
       " 2003,\n",
       " 1037,\n",
       " 15544,\n",
       " 19307,\n",
       " 1998,\n",
       " 3653,\n",
       " 6528,\n",
       " 20771,\n",
       " 19986,\n",
       " 8632,\n",
       " 1012,\n",
       " 2009,\n",
       " 2987,\n",
       " 1005,\n",
       " 1056,\n",
       " 3043,\n",
       " 2054,\n",
       " 2028,\n",
       " 1005,\n",
       " 1055,\n",
       " 2576,\n",
       " 5328,\n",
       " 2024,\n",
       " 2138,\n",
       " 2023,\n",
       " 2143,\n",
       " 2064,\n",
       " 6684,\n",
       " 2022,\n",
       " 2579,\n",
       " 5667,\n",
       " 2006,\n",
       " 2151,\n",
       " 2504,\n",
       " 1012,\n",
       " 2004,\n",
       " 2005,\n",
       " 1996,\n",
       " 4366,\n",
       " 2008,\n",
       " 19124,\n",
       " 3287,\n",
       " 16371,\n",
       " 25469,\n",
       " 2003,\n",
       " 2019,\n",
       " 6882,\n",
       " 13316,\n",
       " 1011,\n",
       " 2459,\n",
       " 1010,\n",
       " 2008,\n",
       " 3475,\n",
       " 1005,\n",
       " 1056,\n",
       " 2995,\n",
       " 1012,\n",
       " 1045,\n",
       " 1005,\n",
       " 2310,\n",
       " 2464,\n",
       " 1054,\n",
       " 1011,\n",
       " 6758,\n",
       " 3152,\n",
       " 2007,\n",
       " 3287,\n",
       " 16371,\n",
       " 25469,\n",
       " 1012,\n",
       " 4379,\n",
       " 1010,\n",
       " 2027,\n",
       " 2069,\n",
       " 3749,\n",
       " 2070,\n",
       " 25085,\n",
       " 5328,\n",
       " 1010,\n",
       " 2021,\n",
       " 2073,\n",
       " 2024,\n",
       " 1996,\n",
       " 1054,\n",
       " 1011,\n",
       " 6758,\n",
       " 3152,\n",
       " 2007,\n",
       " 21226,\n",
       " 24728,\n",
       " 22144,\n",
       " 2015,\n",
       " 1998,\n",
       " 20916,\n",
       " 4691,\n",
       " 6845,\n",
       " 2401,\n",
       " 1029,\n",
       " 7880,\n",
       " 1010,\n",
       " 2138,\n",
       " 2027,\n",
       " 2123,\n",
       " 1005,\n",
       " 1056,\n",
       " 4839,\n",
       " 1012,\n",
       " 1996,\n",
       " 2168,\n",
       " 3632,\n",
       " 2005,\n",
       " 2216,\n",
       " 10231,\n",
       " 7685,\n",
       " 5830,\n",
       " 3065,\n",
       " 1024,\n",
       " 8040,\n",
       " 7317,\n",
       " 5063,\n",
       " 2015,\n",
       " 11820,\n",
       " 1999,\n",
       " 1996,\n",
       " 9478,\n",
       " 2021,\n",
       " 2025,\n",
       " 1037,\n",
       " 17962,\n",
       " 21239,\n",
       " 1999,\n",
       " 4356,\n",
       " 1012,\n",
       " 1998,\n",
       " 2216,\n",
       " 3653,\n",
       " 6528,\n",
       " 20771,\n",
       " 10271,\n",
       " 5691,\n",
       " 2066,\n",
       " 1996,\n",
       " 2829,\n",
       " 16291,\n",
       " 1010,\n",
       " 1999,\n",
       " 2029,\n",
       " 2057,\n",
       " 1005,\n",
       " 2128,\n",
       " 5845,\n",
       " 2000,\n",
       " 1996,\n",
       " 2609,\n",
       " 1997,\n",
       " 6320,\n",
       " 25624,\n",
       " 1005,\n",
       " 1055,\n",
       " 17061,\n",
       " 3779,\n",
       " 1010,\n",
       " 2021,\n",
       " 2025,\n",
       " 1037,\n",
       " 7637,\n",
       " 1997,\n",
       " 5061,\n",
       " 5710,\n",
       " 2006,\n",
       " 9318,\n",
       " 7367,\n",
       " 5737,\n",
       " 19393,\n",
       " 1012,\n",
       " 2077,\n",
       " 6933,\n",
       " 1006,\n",
       " 2030,\n",
       " 20242,\n",
       " 1007,\n",
       " 1000,\n",
       " 3313,\n",
       " 1011,\n",
       " 3115,\n",
       " 1000,\n",
       " 1999,\n",
       " 5609,\n",
       " 1997,\n",
       " 16371,\n",
       " 25469,\n",
       " 1010,\n",
       " 1996,\n",
       " 10597,\n",
       " 27885,\n",
       " 5809,\n",
       " 2063,\n",
       " 2323,\n",
       " 2202,\n",
       " 2046,\n",
       " 4070,\n",
       " 2028,\n",
       " 14477,\n",
       " 6767,\n",
       " 8524,\n",
       " 6321,\n",
       " 5793,\n",
       " 28141,\n",
       " 4489,\n",
       " 2090,\n",
       " 2273,\n",
       " 1998,\n",
       " 2308,\n",
       " 1024,\n",
       " 2045,\n",
       " 2024,\n",
       " 2053,\n",
       " 8991,\n",
       " 18400,\n",
       " 2015,\n",
       " 2006,\n",
       " 4653,\n",
       " 2043,\n",
       " 19910,\n",
       " 3544,\n",
       " 15287,\n",
       " 1010,\n",
       " 1998,\n",
       " 1996,\n",
       " 2168,\n",
       " 3685,\n",
       " 2022,\n",
       " 2056,\n",
       " 2005,\n",
       " 1037,\n",
       " 2158,\n",
       " 1012,\n",
       " 1999,\n",
       " 2755,\n",
       " 1010,\n",
       " 2017,\n",
       " 3227,\n",
       " 2180,\n",
       " 1005,\n",
       " 1056,\n",
       " 2156,\n",
       " 2931,\n",
       " 8991,\n",
       " 18400,\n",
       " 2015,\n",
       " 1999,\n",
       " 2019,\n",
       " 2137,\n",
       " 2143,\n",
       " 1999,\n",
       " 2505,\n",
       " 2460,\n",
       " 1997,\n",
       " 22555,\n",
       " 2030,\n",
       " 13216,\n",
       " 14253,\n",
       " 2050,\n",
       " 1012,\n",
       " 2023,\n",
       " 6884,\n",
       " 3313,\n",
       " 1011,\n",
       " 3115,\n",
       " 2003,\n",
       " 2625,\n",
       " 1037,\n",
       " 3313,\n",
       " 3115,\n",
       " 2084,\n",
       " 2019,\n",
       " 4914,\n",
       " 2135,\n",
       " 2139,\n",
       " 24128,\n",
       " 3754,\n",
       " 2000,\n",
       " 2272,\n",
       " 2000,\n",
       " 3408,\n",
       " 20547,\n",
       " 2007,\n",
       " 1996,\n",
       " 19008,\n",
       " 1997,\n",
       " 2308,\n",
       " 1005,\n",
       " 1055,\n",
       " 4230,\n",
       " 1012,\n",
       " 102,\n",
       " 101,\n",
       " 2065,\n",
       " 2069,\n",
       " 2000,\n",
       " 4468,\n",
       " 2437,\n",
       " 2023,\n",
       " 2828,\n",
       " 1997,\n",
       " 2143,\n",
       " 1999,\n",
       " 1996,\n",
       " 2925,\n",
       " 1012,\n",
       " 2023,\n",
       " 2143,\n",
       " 2003,\n",
       " 5875,\n",
       " 2004,\n",
       " 2019,\n",
       " 7551,\n",
       " 2021,\n",
       " 4136,\n",
       " 2053,\n",
       " 2522,\n",
       " 11461,\n",
       " 2466,\n",
       " 1012,\n",
       " 1026,\n",
       " 7987,\n",
       " 1013,\n",
       " 1028,\n",
       " 1026,\n",
       " 7987,\n",
       " 1013,\n",
       " 1028,\n",
       " 2028,\n",
       " 2453,\n",
       " 2514,\n",
       " 6819,\n",
       " 5339,\n",
       " 8918,\n",
       " 2005,\n",
       " 3564,\n",
       " 27046,\n",
       " 2009,\n",
       " 2138,\n",
       " 2009,\n",
       " 12817,\n",
       " 2006,\n",
       " 2061,\n",
       " 2116,\n",
       " 2590,\n",
       " 3314,\n",
       " 2021,\n",
       " 2009,\n",
       " 2515,\n",
       " 2061,\n",
       " 2302,\n",
       " 2151,\n",
       " 5860,\n",
       " 11795,\n",
       " 3085,\n",
       " 15793,\n",
       " 1012,\n",
       " 1996,\n",
       " 13972,\n",
       " 3310,\n",
       " 2185,\n",
       " 2007,\n",
       " 2053,\n",
       " 2047,\n",
       " 15251,\n",
       " 1006,\n",
       " 4983,\n",
       " 2028,\n",
       " 3310,\n",
       " 2039,\n",
       " 2007,\n",
       " 2028,\n",
       " 2096,\n",
       " 2028,\n",
       " 1005,\n",
       " 1055,\n",
       " 2568,\n",
       " 17677,\n",
       " 2015,\n",
       " 1010,\n",
       " 2004,\n",
       " 2009,\n",
       " 2097,\n",
       " 26597,\n",
       " 2079,\n",
       " 2076,\n",
       " 2023,\n",
       " 23100,\n",
       " 2143,\n",
       " 1007,\n",
       " 1012,\n",
       " 1026,\n",
       " 7987,\n",
       " 1013,\n",
       " 1028,\n",
       " 1026,\n",
       " 7987,\n",
       " 1013,\n",
       " 1028,\n",
       " 2028,\n",
       " 2453,\n",
       " 2488,\n",
       " 5247,\n",
       " 2028,\n",
       " 1005,\n",
       " 1055,\n",
       " 2051,\n",
       " 4582,\n",
       " 2041,\n",
       " 1037,\n",
       " 3332,\n",
       " 2012,\n",
       " 1037,\n",
       " 3392,\n",
       " 3652,\n",
       " 1012,\n",
       " 1026,\n",
       " 7987,\n",
       " 1013,\n",
       " 1028,\n",
       " 1026,\n",
       " 7987,\n",
       " 1013,\n",
       " 1028,\n",
       " 102]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the concatenation function\n",
    "sum(tokenized_samples[\"input_ids\"], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Concatenated reviews length: 800\n"
     ]
    }
   ],
   "source": [
    "# testing: the concatenated length should be 800, for these 3 sequences in this sample:\n",
    "# >>> Review 0 length: 363\n",
    "# >>> Review 1 length: 304\n",
    "# >>> Review 2 length: 133\n",
    "concatenated_examples = {\n",
    "    k: sum(tokenized_samples[k], []) for k in tokenized_samples.keys()\n",
    "}\n",
    "total_length = len(concatenated_examples[\"input_ids\"])\n",
    "print(f\">>> Concatenated reviews length: {total_length}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of dividing the big concatenated sequence into equal chunks, there may be a short chunk left over, like in the example below. We will discard it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 1045, 12524, 1045, 2572, 8025, 1011, 3756, 2013, 2026, 2678, 3573, 2138, 1997, 2035, 1996, 6704, 2008, 5129, 2009, 2043, 2009, 2001, 2034, 2207, 1999, 3476, 1012, 1045, 2036, 2657, 2008, 2012, 2034, 2009, 2001, 8243, 2011, 1057, 1012, 1055, 1012, 8205, 2065, 2009, 2412, 2699, 2000, 4607, 2023, 2406, 1010, 3568, 2108, 1037, 5470, 1997, 3152, 2641, 1000, 6801, 1000, 1045, 2428, 2018, 2000, 2156, 2023, 2005, 2870, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 5436, 2003, 8857, 2105, 1037, 2402, 4467, 3689, 3076, 2315, 14229, 2040, 4122, 2000, 4553, 2673, 2016, 2064, 2055, 2166, 1012, 1999, 3327, 2016, 4122, 2000, 3579, 2014, 3086, 2015, 2000, 2437, 2070, 4066, 1997, 4516, 2006, 2054, 1996, 2779, 25430, 14728, 2245, 2055, 3056, 2576, 3314, 2107], [2004, 1996, 5148, 2162, 1998, 2679, 3314, 1999, 1996, 2142, 2163, 1012, 1999, 2090, 4851, 8801, 1998, 6623, 7939, 4697, 3619, 1997, 8947, 2055, 2037, 10740, 2006, 4331, 1010, 2016, 2038, 3348, 2007, 2014, 3689, 3836, 1010, 19846, 1010, 1998, 2496, 2273, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2054, 8563, 2033, 2055, 1045, 2572, 8025, 1011, 3756, 2003, 2008, 2871, 2086, 3283, 1010, 2023, 2001, 2641, 26932, 1012, 2428, 1010, 1996, 3348, 1998, 16371, 25469, 5019, 2024, 2261, 1998, 2521, 2090, 1010, 2130, 2059, 2009, 1005, 1055, 2025, 2915, 2066, 2070, 10036, 2135, 2081, 22555, 2080, 1012, 2096, 2026, 2406, 3549, 2568, 2424, 2009, 16880, 1010, 1999, 4507, 3348, 1998, 16371, 25469, 2024, 1037, 2350, 18785, 1999, 4467, 5988, 1012, 2130, 13749, 7849, 24544, 1010], [15835, 2037, 3437, 2000, 2204, 2214, 2879, 2198, 4811, 1010, 2018, 3348, 5019, 1999, 2010, 3152, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1045, 2079, 4012, 3549, 2094, 1996, 16587, 2005, 1996, 2755, 2008, 2151, 3348, 3491, 1999, 1996, 2143, 2003, 3491, 2005, 6018, 5682, 2738, 2084, 2074, 2000, 5213, 2111, 1998, 2191, 2769, 2000, 2022, 3491, 1999, 26932, 12370, 1999, 2637, 1012, 1045, 2572, 8025, 1011, 3756, 2003, 1037, 2204, 2143, 2005, 3087, 5782, 2000, 2817, 1996, 6240, 1998, 14629, 1006, 2053, 26136, 3832, 1007, 1997, 4467, 5988, 1012, 2021, 2428, 1010, 2023, 2143, 2987, 1005, 1056, 2031, 2172, 1997, 1037, 5436, 1012, 102, 101, 1000, 1045, 2572, 8025, 1024, 3756, 1000, 2003, 1037, 15544, 19307, 1998, 3653, 6528, 20771, 19986, 8632, 1012, 2009, 2987], [1005, 1056, 3043, 2054, 2028, 1005, 1055, 2576, 5328, 2024, 2138, 2023, 2143, 2064, 6684, 2022, 2579, 5667, 2006, 2151, 2504, 1012, 2004, 2005, 1996, 4366, 2008, 19124, 3287, 16371, 25469, 2003, 2019, 6882, 13316, 1011, 2459, 1010, 2008, 3475, 1005, 1056, 2995, 1012, 1045, 1005, 2310, 2464, 1054, 1011, 6758, 3152, 2007, 3287, 16371, 25469, 1012, 4379, 1010, 2027, 2069, 3749, 2070, 25085, 5328, 1010, 2021, 2073, 2024, 1996, 1054, 1011, 6758, 3152, 2007, 21226, 24728, 22144, 2015, 1998, 20916, 4691, 6845, 2401, 1029, 7880, 1010, 2138, 2027, 2123, 1005, 1056, 4839, 1012, 1996, 2168, 3632, 2005, 2216, 10231, 7685, 5830, 3065, 1024, 8040, 7317, 5063, 2015, 11820, 1999, 1996, 9478, 2021, 2025, 1037, 17962, 21239, 1999, 4356, 1012, 1998, 2216, 3653, 6528, 20771, 10271, 5691, 2066], [1996, 2829, 16291, 1010, 1999, 2029, 2057, 1005, 2128, 5845, 2000, 1996, 2609, 1997, 6320, 25624, 1005, 1055, 17061, 3779, 1010, 2021, 2025, 1037, 7637, 1997, 5061, 5710, 2006, 9318, 7367, 5737, 19393, 1012, 2077, 6933, 1006, 2030, 20242, 1007, 1000, 3313, 1011, 3115, 1000, 1999, 5609, 1997, 16371, 25469, 1010, 1996, 10597, 27885, 5809, 2063, 2323, 2202, 2046, 4070, 2028, 14477, 6767, 8524, 6321, 5793, 28141, 4489, 2090, 2273, 1998, 2308, 1024, 2045, 2024, 2053, 8991, 18400, 2015, 2006, 4653, 2043, 19910, 3544, 15287, 1010, 1998, 1996, 2168, 3685, 2022, 2056, 2005, 1037, 2158, 1012, 1999, 2755, 1010, 2017, 3227, 2180, 1005, 1056, 2156, 2931, 8991, 18400, 2015, 1999, 2019, 2137, 2143, 1999, 2505, 2460, 1997, 22555, 2030, 13216, 14253, 2050, 1012, 2023, 6884, 3313, 1011, 3115], [2003, 2625, 1037, 3313, 3115, 2084, 2019, 4914, 2135, 2139, 24128, 3754, 2000, 2272, 2000, 3408, 20547, 2007, 1996, 19008, 1997, 2308, 1005, 1055, 4230, 1012, 102, 101, 2065, 2069, 2000, 4468, 2437, 2023, 2828, 1997, 2143, 1999, 1996, 2925, 1012, 2023, 2143, 2003, 5875, 2004, 2019, 7551, 2021, 4136, 2053, 2522, 11461, 2466, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2028, 2453, 2514, 6819, 5339, 8918, 2005, 3564, 27046, 2009, 2138, 2009, 12817, 2006, 2061, 2116, 2590, 3314, 2021, 2009, 2515, 2061, 2302, 2151, 5860, 11795, 3085, 15793, 1012, 1996, 13972, 3310, 2185, 2007, 2053, 2047, 15251, 1006, 4983, 2028, 3310, 2039, 2007, 2028, 2096, 2028, 1005, 1055, 2568, 17677, 2015, 1010, 2004, 2009, 2097, 26597, 2079, 2076, 2023, 23100, 2143, 1007, 1012, 1026, 7987], [1013, 1028, 1026, 7987, 1013, 1028, 2028, 2453, 2488, 5247, 2028, 1005, 1055, 2051, 4582, 2041, 1037, 3332, 2012, 1037, 3392, 3652, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'word_ids': [[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 118, 119, 120, 121, 122, 123, 124], [125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 143, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 216, 217, 218, 218, 219, 220, 221, 222, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 242, 243, 244], [245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 272, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, None, None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 9, 10, 11, 11, 11, 12, 13, 14, 15, 16], [17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 91, 91, 92, 93, 93, 94, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 110, 111, 112, 113, 114, 114, 114, 114, 115, 116, 117, 118, 119, 120, 121, 122, 122, 123, 124, 125, 126, 127, 128, 128, 128, 129, 130, 131], [132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 162, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 178, 179, 180, 181, 182, 182, 182, 183, 184, 185, 186, 187, 188, 188, 188, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 200, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 228, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 240, 241, 242, 243, 244, 245, 246], [247, 248, 249, 250, 251, 252, 253, 254, 254, 255, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, None, None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 37, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 56, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93], [94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, None]]}\n",
      ">>> Chunk length: 128\n",
      ">>> Chunk length: 128\n",
      ">>> Chunk length: 128\n",
      ">>> Chunk length: 128\n",
      ">>> Chunk length: 128\n",
      ">>> Chunk length: 128\n",
      ">>> Chunk length: 32\n"
     ]
    }
   ],
   "source": [
    "chunks = {\n",
    "    k: [t[i: i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "    for k, t in concatenated_examples.items()\n",
    "}\n",
    "print(chunks)\n",
    "for chunk in chunks[\"input_ids\"]:\n",
    "    print(f\">>> Chunk length: {len(chunk)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function combines the steps explained above. Again, this code comes from the Hugging Face tutorial.\n",
    "\n",
    "In this function, we create a new \"labels\" column - an exact copy of the \"input_ids\" column. We will later mask parts of \"input_ids\", so this clone stored in \"labels\" will be the ground-truth for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    # concatenate all texts\n",
    "    concatenated_examples = {\n",
    "        k: sum(examples[k], []) for k in examples.keys()\n",
    "    }\n",
    "    # compute length of concatenated texts\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # we drop the last chunk if it's smaller than chunk_size\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "    \n",
    "    # split by chunks of max_len\n",
    "    result = {\n",
    "        k: [t[i: i + chunk_size] for i in range(0, total_length, chunk_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    \n",
    "    # create a new labels column - same as the input_ids. We will later mask parts of \"input_ids\", so the clone stored in \"labels\" will be the ground-truth for training\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_datasets = tokenized_datasets.map(group_texts, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"as the vietnam war and race issues in the united states. in between asking politicians and ordinary denizens of stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men. < br / > < br / > what kills me about i am curious - yellow is that 40 years ago, this was considered pornographic. really, the sex and nudity scenes are few and far between, even then it's not shot like some cheaply made porno. while my countrymen mind find it shocking, in reality sex and nudity are a major staple in swedish cinema. even ingmar bergman,\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(lm_datasets[\"train\"][1][\"input_ids\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"as the vietnam war and race issues in the united states. in between asking politicians and ordinary denizens of stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men. < br / > < br / > what kills me about i am curious - yellow is that 40 years ago, this was considered pornographic. really, the sex and nudity scenes are few and far between, even then it's not shot like some cheaply made porno. while my countrymen mind find it shocking, in reality sex and nudity are a major staple in swedish cinema. even ingmar bergman,\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(lm_datasets[\"train\"][1][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 61291\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 59904\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 122957\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the *DataCollatorForLanguageModeling* - not be used here since we want \"whole word masking\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples: [{'input_ids': [101, 1045, 12524, 1045, 2572, 8025, 1011, 3756, 2013, 2026, 2678, 3573, 2138, 1997, 2035, 1996, 6704, 2008, 5129, 2009, 2043, 2009, 2001, 2034, 2207, 1999, 3476, 1012, 1045, 2036, 2657, 2008, 2012, 2034, 2009, 2001, 8243, 2011, 1057, 1012, 1055, 1012, 8205, 2065, 2009, 2412, 2699, 2000, 4607, 2023, 2406, 1010, 3568, 2108, 1037, 5470, 1997, 3152, 2641, 1000, 6801, 1000, 1045, 2428, 2018, 2000, 2156, 2023, 2005, 2870, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 5436, 2003, 8857, 2105, 1037, 2402, 4467, 3689, 3076, 2315, 14229, 2040, 4122, 2000, 4553, 2673, 2016, 2064, 2055, 2166, 1012, 1999, 3327, 2016, 4122, 2000, 3579, 2014, 3086, 2015, 2000, 2437, 2070, 4066, 1997, 4516, 2006, 2054, 1996, 2779, 25430, 14728, 2245, 2055, 3056, 2576, 3314, 2107], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'word_ids': [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 118, 119, 120, 121, 122, 123, 124], 'labels': [101, 1045, 12524, 1045, 2572, 8025, 1011, 3756, 2013, 2026, 2678, 3573, 2138, 1997, 2035, 1996, 6704, 2008, 5129, 2009, 2043, 2009, 2001, 2034, 2207, 1999, 3476, 1012, 1045, 2036, 2657, 2008, 2012, 2034, 2009, 2001, 8243, 2011, 1057, 1012, 1055, 1012, 8205, 2065, 2009, 2412, 2699, 2000, 4607, 2023, 2406, 1010, 3568, 2108, 1037, 5470, 1997, 3152, 2641, 1000, 6801, 1000, 1045, 2428, 2018, 2000, 2156, 2023, 2005, 2870, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 5436, 2003, 8857, 2105, 1037, 2402, 4467, 3689, 3076, 2315, 14229, 2040, 4122, 2000, 4553, 2673, 2016, 2064, 2055, 2166, 1012, 1999, 3327, 2016, 4122, 2000, 3579, 2014, 3086, 2015, 2000, 2437, 2070, 4066, 1997, 4516, 2006, 2054, 1996, 2779, 25430, 14728, 2245, 2055, 3056, 2576, 3314, 2107]}, {'input_ids': [2004, 1996, 5148, 2162, 1998, 2679, 3314, 1999, 1996, 2142, 2163, 1012, 1999, 2090, 4851, 8801, 1998, 6623, 7939, 4697, 3619, 1997, 8947, 2055, 2037, 10740, 2006, 4331, 1010, 2016, 2038, 3348, 2007, 2014, 3689, 3836, 1010, 19846, 1010, 1998, 2496, 2273, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2054, 8563, 2033, 2055, 1045, 2572, 8025, 1011, 3756, 2003, 2008, 2871, 2086, 3283, 1010, 2023, 2001, 2641, 26932, 1012, 2428, 1010, 1996, 3348, 1998, 16371, 25469, 5019, 2024, 2261, 1998, 2521, 2090, 1010, 2130, 2059, 2009, 1005, 1055, 2025, 2915, 2066, 2070, 10036, 2135, 2081, 22555, 2080, 1012, 2096, 2026, 2406, 3549, 2568, 2424, 2009, 16880, 1010, 1999, 4507, 3348, 1998, 16371, 25469, 2024, 1037, 2350, 18785, 1999, 4467, 5988, 1012, 2130, 13749, 7849, 24544, 1010], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'word_ids': [125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 143, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 216, 217, 218, 218, 219, 220, 221, 222, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 242, 243, 244], 'labels': [2004, 1996, 5148, 2162, 1998, 2679, 3314, 1999, 1996, 2142, 2163, 1012, 1999, 2090, 4851, 8801, 1998, 6623, 7939, 4697, 3619, 1997, 8947, 2055, 2037, 10740, 2006, 4331, 1010, 2016, 2038, 3348, 2007, 2014, 3689, 3836, 1010, 19846, 1010, 1998, 2496, 2273, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2054, 8563, 2033, 2055, 1045, 2572, 8025, 1011, 3756, 2003, 2008, 2871, 2086, 3283, 1010, 2023, 2001, 2641, 26932, 1012, 2428, 1010, 1996, 3348, 1998, 16371, 25469, 5019, 2024, 2261, 1998, 2521, 2090, 1010, 2130, 2059, 2009, 1005, 1055, 2025, 2915, 2066, 2070, 10036, 2135, 2081, 22555, 2080, 1012, 2096, 2026, 2406, 3549, 2568, 2424, 2009, 16880, 1010, 1999, 4507, 3348, 1998, 16371, 25469, 2024, 1037, 2350, 18785, 1999, 4467, 5988, 1012, 2130, 13749, 7849, 24544, 1010]}]\n",
      "samples: [{'input_ids': [101, 1045, 12524, 1045, 2572, 8025, 1011, 3756, 2013, 2026, 2678, 3573, 2138, 1997, 2035, 1996, 6704, 2008, 5129, 2009, 2043, 2009, 2001, 2034, 2207, 1999, 3476, 1012, 1045, 2036, 2657, 2008, 2012, 2034, 2009, 2001, 8243, 2011, 1057, 1012, 1055, 1012, 8205, 2065, 2009, 2412, 2699, 2000, 4607, 2023, 2406, 1010, 3568, 2108, 1037, 5470, 1997, 3152, 2641, 1000, 6801, 1000, 1045, 2428, 2018, 2000, 2156, 2023, 2005, 2870, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 5436, 2003, 8857, 2105, 1037, 2402, 4467, 3689, 3076, 2315, 14229, 2040, 4122, 2000, 4553, 2673, 2016, 2064, 2055, 2166, 1012, 1999, 3327, 2016, 4122, 2000, 3579, 2014, 3086, 2015, 2000, 2437, 2070, 4066, 1997, 4516, 2006, 2054, 1996, 2779, 25430, 14728, 2245, 2055, 3056, 2576, 3314, 2107], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [101, 1045, 12524, 1045, 2572, 8025, 1011, 3756, 2013, 2026, 2678, 3573, 2138, 1997, 2035, 1996, 6704, 2008, 5129, 2009, 2043, 2009, 2001, 2034, 2207, 1999, 3476, 1012, 1045, 2036, 2657, 2008, 2012, 2034, 2009, 2001, 8243, 2011, 1057, 1012, 1055, 1012, 8205, 2065, 2009, 2412, 2699, 2000, 4607, 2023, 2406, 1010, 3568, 2108, 1037, 5470, 1997, 3152, 2641, 1000, 6801, 1000, 1045, 2428, 2018, 2000, 2156, 2023, 2005, 2870, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 5436, 2003, 8857, 2105, 1037, 2402, 4467, 3689, 3076, 2315, 14229, 2040, 4122, 2000, 4553, 2673, 2016, 2064, 2055, 2166, 1012, 1999, 3327, 2016, 4122, 2000, 3579, 2014, 3086, 2015, 2000, 2437, 2070, 4066, 1997, 4516, 2006, 2054, 1996, 2779, 25430, 14728, 2245, 2055, 3056, 2576, 3314, 2107]}, {'input_ids': [2004, 1996, 5148, 2162, 1998, 2679, 3314, 1999, 1996, 2142, 2163, 1012, 1999, 2090, 4851, 8801, 1998, 6623, 7939, 4697, 3619, 1997, 8947, 2055, 2037, 10740, 2006, 4331, 1010, 2016, 2038, 3348, 2007, 2014, 3689, 3836, 1010, 19846, 1010, 1998, 2496, 2273, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2054, 8563, 2033, 2055, 1045, 2572, 8025, 1011, 3756, 2003, 2008, 2871, 2086, 3283, 1010, 2023, 2001, 2641, 26932, 1012, 2428, 1010, 1996, 3348, 1998, 16371, 25469, 5019, 2024, 2261, 1998, 2521, 2090, 1010, 2130, 2059, 2009, 1005, 1055, 2025, 2915, 2066, 2070, 10036, 2135, 2081, 22555, 2080, 1012, 2096, 2026, 2406, 3549, 2568, 2424, 2009, 16880, 1010, 1999, 4507, 3348, 1998, 16371, 25469, 2024, 1037, 2350, 18785, 1999, 4467, 5988, 1012, 2130, 13749, 7849, 24544, 1010], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [2004, 1996, 5148, 2162, 1998, 2679, 3314, 1999, 1996, 2142, 2163, 1012, 1999, 2090, 4851, 8801, 1998, 6623, 7939, 4697, 3619, 1997, 8947, 2055, 2037, 10740, 2006, 4331, 1010, 2016, 2038, 3348, 2007, 2014, 3689, 3836, 1010, 19846, 1010, 1998, 2496, 2273, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2054, 8563, 2033, 2055, 1045, 2572, 8025, 1011, 3756, 2003, 2008, 2871, 2086, 3283, 1010, 2023, 2001, 2641, 26932, 1012, 2428, 1010, 1996, 3348, 1998, 16371, 25469, 5019, 2024, 2261, 1998, 2521, 2090, 1010, 2130, 2059, 2009, 1005, 1055, 2025, 2915, 2066, 2070, 10036, 2135, 2081, 22555, 2080, 1012, 2096, 2026, 2406, 3549, 2568, 2424, 2009, 16880, 1010, 1999, 4507, 3348, 1998, 16371, 25469, 2024, 1037, 2350, 18785, 1999, 4467, 5988, 1012, 2130, 13749, 7849, 24544, 1010]}]\n",
      "chunk = tensor([  101,  1045, 12524,   103,  2572,  8025,  1011,  3756,  2013,  2026,\n",
      "         2678,  3573,  2138,  1997,  2035,  1996,  6704,  2008,  5129,   103,\n",
      "         2043,  2009,   103,  2034,  2207,  1999,   103,  1012,  1045,  2036,\n",
      "         2657,  2008,  2012,  2034,  2009,  2001,  8243,  2011,  1057,  1012,\n",
      "         1055,  1012,   103,  2065,  2009,  2412,  2699,  2000,  4607,  2023,\n",
      "         2406,  1010,  3568,  2108,  1037,  5470,  1997,  3152,  2641,  1000,\n",
      "         6801,  1000,  1045,   103,  2018,  2000,  2156,  2023,  2005,  2870,\n",
      "          103,  1026,  7987,  1013,  1028,  1026,  7987,  1013,  1028,  1996,\n",
      "          103,  2003,  8857,  2105,  1037,  2402,   103,  3689,   103,  2315,\n",
      "          103,  2040,  4122,  2000,  4553,  2673,  2016,  2064,   103,  2166,\n",
      "         1012,  1999,  3327,  2016,  4122,  2000,   103,  2014,  3086,  2015,\n",
      "         2000,  2437,  2070,   103,  1997,  4516,  2006,  2054,  1996,  2779,\n",
      "        25430, 14728,  2245,  2055,  3056,  2576,  3314,  2107])\n",
      ">>> [CLS] i rented [MASK] am curious - yellow from my video store because of all the controversy that surrounded [MASK] when it [MASK] first released in [MASK]. i also heard that at first it was seized by u. s. [MASK] if it ever tried to enter this country, therefore being a fan of films considered \" controversial \" i [MASK] had to see this for myself [MASK] < br / > < br / > the [MASK] is centered around a young [MASK] drama [MASK] named [MASK] who wants to learn everything she can [MASK] life. in particular she wants to [MASK] her attentions to making some [MASK] of documentary on what the average swede thought about certain political issues such\n",
      "\n",
      "chunk = tensor([ 2004,   103,  5148,  2162,  1998,  2679,  3314,  1999,  1996,  2142,\n",
      "         2163,  1012,   103,  2090,  4851,   103,  1998,  6623,  7939,  4697,\n",
      "         3619,  1997,  8947,  2055,  2037,   103,  2006,  4331,   103,  2016,\n",
      "         2038,  3348,  2007,   103,   103,  3836,   103, 19846,  1010,  1998,\n",
      "         2496,  2273,   103,  1026,  7987,  1013,   103,  1026,  7987,  1013,\n",
      "         1028,   103,  8563,  2033,  2055,  1045,  2572,  8025,  1011,  3756,\n",
      "         2003,   103,  2871,   103,  3283,  1010,  2023,  2001,   103, 26932,\n",
      "         1012,  2428,  1010,  1996,  3348,  1998, 16371, 25469,  5019,  2024,\n",
      "         2261,  1998,  2521,  2090,  1010,  2130,  2059,  2009,  1005,  1055,\n",
      "         2025,  2915,  2066,  2070, 10036,   103,  2081, 22555,  2080,   103,\n",
      "         2096,  2026,   103,   103,   103,  2424,  2009, 16880,  1010,  1999,\n",
      "          103,  3348,   103, 16371, 25469,  2024,   103,  2350, 18785,  1999,\n",
      "         4467,  5988,  1012,  2130, 13749,  7849, 24544,  1010])\n",
      ">>> as [MASK] vietnam war and race issues in the united states. [MASK] between asking [MASK] and ordinary denizens of stockholm about their [MASK] on politics [MASK] she has sex with [MASK] [MASK] teacher [MASK] classmates, and married men [MASK] < br / [MASK] < br / > [MASK] kills me about i am curious - yellow is [MASK] 40 [MASK] ago, this was [MASK] pornographic. really, the sex and nudity scenes are few and far between, even then it's not shot like some cheap [MASK] made porno [MASK] while my [MASK] [MASK] [MASK] find it shocking, in [MASK] sex [MASK] nudity are [MASK] major staple in swedish cinema. even ingmar bergman,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "samples = [lm_datasets[\"train\"][i] for i in range(2)]\n",
    "print(f\"samples: {samples}\")\n",
    "\n",
    "# We remove the \"word_ids\" key for this data collator as it does not expect it:\n",
    "for sample in samples:\n",
    "    _ = sample.pop(\"word_ids\")\n",
    "    \n",
    "print(f\"samples: {samples}\")\n",
    "\n",
    "for chunk in data_collator(samples)[\"input_ids\"]:\n",
    "    print(f\"chunk = {chunk}\")\n",
    "    print(f\">>> {tokenizer.decode(chunk)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whole word masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "from transformers import default_data_collator\n",
    "\n",
    "wwm_probability = 0.2\n",
    "\n",
    "def whole_word_masking_data_collator(features):\n",
    "    for feature in features:\n",
    "        word_ids = feature.pop(\"word_ids\")\n",
    "        \n",
    "        # Create a map between words and corresponding token indices\n",
    "        mapping = collections.defaultdict(list)\n",
    "        current_word_index = -1\n",
    "        current_word = None\n",
    "        for idx, word_id in enumerate(word_ids):\n",
    "            if word_id is not None:\n",
    "                if word_id != current_word:\n",
    "                    current_word = word_id\n",
    "                    current_word_index += 1\n",
    "                mapping[current_word_index].append(idx)\n",
    "                \n",
    "        # randomly mask words\n",
    "        mask = np.random.binomial(1, wwm_probability, (len(mapping), ))\n",
    "        input_ids = feature[\"input_ids\"]\n",
    "        labels = feature[\"labels\"]\n",
    "        new_labels = [-100] * len(labels)\n",
    "        for word_id in np.where(mask)[0]:\n",
    "            word_id = word_id.item()  # TODO\n",
    "            for idx in mapping[word_id]:\n",
    "                new_labels[idx] = labels[idx]\n",
    "                input_ids[idx] = tokenizer.mask_token_id\n",
    "                \n",
    "        feature[\"labels\"] = new_labels\n",
    "    ret = default_data_collator(features)\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> [CLS] [MASK] rented i am curious - [MASK] from my video store because of all the controversy that surrounded it when [MASK] was [MASK] released [MASK] 1967. i [MASK] heard that at first it was seized by u. [MASK]. customs if [MASK] ever [MASK] to enter this country, therefore being [MASK] [MASK] of films considered \" controversial \" i really had to see this for myself. < br / > [MASK] br / > the plot is [MASK] around a young swedish drama [MASK] named lena who wants to learn [MASK] she can about life [MASK] [MASK] particular she wants [MASK] focus her attentions to making some sort of [MASK] on what the average swede [MASK] [MASK] certain political [MASK] [MASK]\n",
      "\n",
      ">>> as [MASK] [MASK] war and race issues in the united states. in between asking [MASK] and ordinary denizens of stockholm about their opinions on politics, [MASK] [MASK] sex with her drama teacher, classmates, [MASK] married men. < br / > [MASK] br / > what kills me about i am curious - yellow [MASK] that 40 [MASK] ago, this was [MASK] pornographic [MASK] really, [MASK] sex and nudity scenes [MASK] few [MASK] far between, even then it [MASK] s [MASK] shot like [MASK] cheaply made porno. [MASK] my [MASK] [MASK] [MASK] [MASK] it shocking, in reality [MASK] and [MASK] [MASK] are [MASK] major staple in swedish cinema. even ingmar bergman,\n"
     ]
    }
   ],
   "source": [
    "samples = [lm_datasets[\"train\"][i] for i in range(2)]\n",
    "batch = whole_word_masking_data_collator(samples)\n",
    "\n",
    "for chunk in batch[\"input_ids\"]:\n",
    "    print(f\"\\n>>> {tokenizer.decode(chunk)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to GPU compute constraint, we will perform this experiment on a small subset of the *imdb* dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'word_ids', 'labels'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = 10_000\n",
    "test_size = int(0.1 * train_size)\n",
    "\n",
    "downsampled_dataset = lm_datasets[\"train\"].train_test_split(train_size=train_size, test_size = test_size, seed=42)\n",
    "downsampled_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune the model using Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2528aa78a75f49deb544549bfd65398f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FYI\n",
    "len(downsampled_dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nguyenthuan49/anaconda3/envs/LLM-finetuning-Coursera/lib/python3.12/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "batch_size = 64\n",
    "# Show the training loss with every epoch\n",
    "\n",
    "# to ensure we track the training loss with each epoch.\n",
    "logging_steps = len(downsampled_dataset[\"train\"]) // batch_size\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{model_name}-finetuned-imdb-HF-tutorial-using-trainer\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    push_to_hub=True,  #  By default, the repository used will be in your namespace and named after the output directory you set\n",
    "    fp16=True,  # used fp16=True to enable mixed-precision training, which gives us another boost in speed\n",
    "    logging_steps=logging_steps,\n",
    "    remove_unused_columns=False,  #  By default, the Trainer will remove any columns that are not part of the model’s forward() method. This means that if you’re using the whole word masking collator, you’ll also need to set remove_unused_columns=False to ensure we don’t lose the word_ids column during training.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=downsampled_dataset[\"train\"],\n",
    "    eval_dataset=downsampled_dataset[\"test\"],\n",
    "    data_collator=whole_word_masking_data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before finetuning, evaluation on the *imdb* `eval_dataset` showed high perplexity. We will see this perplexity decreases on this `eval_dataset` after the model is finetuned on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 03:20]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 62.48\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\">>> Perplexity: {math.exp(eval_results[\"eval_loss\"]):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 4.134828090667725,\n",
       " 'eval_runtime': 9.112,\n",
       " 'eval_samples_per_second': 109.745,\n",
       " 'eval_steps_per_second': 1.756}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='471' max='471' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [471/471 02:56, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.559100</td>\n",
       "      <td>3.321807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.408500</td>\n",
       "      <td>3.286242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.369600</td>\n",
       "      <td>3.279640</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=471, training_loss=3.4457162646477895, metrics={'train_runtime': 180.0438, 'train_samples_per_second': 166.626, 'train_steps_per_second': 2.616, 'total_flos': 994208670720000.0, 'train_loss': 3.4457162646477895, 'epoch': 3.0})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After quick training steps with small subset of data, perplexity did go down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Perplexity: 27.29\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can try to predict on \"This is a great [MASK].\" As noted in the HuggingFace tutorial, the model's predictions are now a little more aligned to the language of the *imdb* movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask_token_index = tensor([5], device='cuda:0')\n",
      "mask_token_logits.shape = torch.Size([1, 30522])\n",
      "'>>> This is a great film.'\n",
      "'>>> This is a great movie.'\n",
      "'>>> This is a great idea.'\n",
      "'>>> This is a great adventure.'\n",
      "'>>> This is a great one.'\n"
     ]
    }
   ],
   "source": [
    "text = \"This is a great [MASK].\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "inputs = inputs.to(model.device)\n",
    "\n",
    "# output of the model is logits for each possible next token, for each position in the sequence\n",
    "token_logits = model(**inputs).logits\n",
    "\n",
    "# Find the location of [MASK] and extract its logits\n",
    "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "print(f\"mask_token_index = {mask_token_index}\")\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "print(f\"mask_token_logits.shape = {mask_token_logits.shape}\")\n",
    "\n",
    "# pick the [MASK] candidates with the highest logits\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "for token in top_5_tokens:\n",
    "    print(f\"'>>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning using `accelerate`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The technique presented in the HF tutorial was to apply the masking once on the whole test set, thus eliminating the randomness in the evaluation stage. First, here is a function that applies the masking on a batch:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_random_mask(batch):\n",
    "    features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n",
    "    masked_inputs = whole_word_masking_data_collator(features)\n",
    "    return {\n",
    "        \"masked_\" + k: v.numpy() for k, v in masked_inputs.items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare everything for training with the Accelerator object: model, train and eval loaders, optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsampled_dataset = downsampled_dataset.remove_columns([\"word_ids\"])\n",
    "\n",
    "# apply this function to our test set and drop the unmasked columns so we can replace them with the masked ones.\n",
    "eval_dataset = downsampled_dataset[\"test\"].map(\n",
    "    insert_random_mask,\n",
    "    batched=True,\n",
    "    remove_columns=downsampled_dataset[\"test\"].column_names,\n",
    ")\n",
    "eval_dataset = eval_dataset.rename_columns(\n",
    "    {\n",
    "        \"masked_input_ids\": \"input_ids\",\n",
    "        \"masked_attention_mask\": \"attention_mask\",\n",
    "        \"masked_labels\": \"labels\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "\n",
    "batch_size = 64\n",
    "train_dataloader = DataLoader(\n",
    "    downsampled_dataset[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=whole_word_masking_data_collator,\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset, batch_size=batch_size, collate_fn=default_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# follow the standard steps with 🤗 Accelerate. First, load a fresh version of the pretrained model:\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_checkpoint)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this fresh model, let's check the predictions again. As expected, the top 5 predicted tokens to replace [MASK] are quite generic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask_token_index = tensor([5])\n",
      "mask_token_logits.shape = torch.Size([1, 30522])\n",
      "'>>> This is a great deal.'\n",
      "'>>> This is a great success.'\n",
      "'>>> This is a great adventure.'\n",
      "'>>> This is a great idea.'\n",
      "'>>> This is a great feat.'\n"
     ]
    }
   ],
   "source": [
    "text = \"This is a great [MASK].\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "inputs = inputs.to(model.device)\n",
    "\n",
    "# output of the model is logits for each possible next token, for each position in the sequence\n",
    "token_logits = model(**inputs).logits\n",
    "\n",
    "# Find the location of [MASK] and extract its logits\n",
    "mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n",
    "print(f\"mask_token_index = {mask_token_index}\")\n",
    "mask_token_logits = token_logits[0, mask_token_index, :]\n",
    "print(f\"mask_token_logits.shape = {mask_token_logits.shape}\")\n",
    "\n",
    "# pick the [MASK] candidates with the highest logits\n",
    "top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n",
    "\n",
    "for token in top_5_tokens:\n",
    "    print(f\"'>>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "num_train_epochs = 3\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thuann2cats/distilbert-base-uncased-finetuned-imdb-HF-tutorial-using-accelerate'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import get_full_repo_name\n",
    "\n",
    "model_name = \"distilbert-base-uncased-finetuned-imdb-HF-tutorial-using-accelerate\"\n",
    "repo_name = get_full_repo_name(model_name)\n",
    "repo_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nguyenthuan49/anaconda3/envs/LLM-finetuning-Coursera/lib/python3.12/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'Repository' (from 'huggingface_hub.repository') is deprecated and will be removed from version '1.0'. Please prefer the http-based alternatives instead. Given its large adoption in legacy code, the complete removal is only planned on next major release.\n",
      "For more details, please read https://huggingface.co/docs/huggingface_hub/concepts/git_vs_http.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "Cloning https://huggingface.co/thuann2cats/distilbert-base-uncased-finetuned-imdb-HF-tutorial-using-accelerate into local empty directory.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import Repository\n",
    "\n",
    "output_dir = model_name\n",
    "repo = Repository(output_dir, clone_from=repo_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard `accelerator` training steps, as used in the HuggingFace tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "619eb8da7422426c963a2a426e0d5524",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/471 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Epoch 0: Perplexity: 27.295514988873794\n",
      ">>> Epoch 1: Perplexity: 25.70487689993093\n",
      ">>> Epoch 2: Perplexity: 25.22046200054443\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import math\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        losses.append(accelerator.gather(loss.repeat(batch_size)))\n",
    "\n",
    "    losses = torch.cat(losses)\n",
    "    losses = losses[: len(eval_dataset)]\n",
    "    try:\n",
    "        perplexity = math.exp(torch.mean(losses))\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "\n",
    "    print(f\">>> Epoch {epoch}: Perplexity: {perplexity}\")\n",
    "\n",
    "    # Save and upload\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        repo.push_to_hub(\n",
    "            commit_message=f\"Training in progress epoch {epoch}\", blocking=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the perplexity went down compared to before finetuning. This is obvious, since the training was the same. We just used `Trainer` vs `Accelerator`.\n",
    "\n",
    "Predictions are more related to movies, as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, since the model was also uploaded as a repository on HuggingFace Hub. We can use `pipeline`: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83b918dd3956459f8d5af612ff9ff9a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/552 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6845bb4544e2408b8982cf39ea9fea97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b73af2cc5515469aa51ebd17d57f64e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb95843e44a746b4bd33d4b3d1a52e08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69d46d8a5a7d4b6a8b54d9832d64d0bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04cdedf065394bbdb1a33e7422f347db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline \n",
    "predictor = pipeline(\"fill-mask\", model=\"thuann2cats/distilbert-base-uncased-finetuned-imdb-HF-tutorial-using-accelerate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> this is a great film.\n",
      ">>> this is a great movie.\n",
      ">>> this is a great idea.\n",
      ">>> this is a great one.\n",
      ">>> this is a great story.\n"
     ]
    }
   ],
   "source": [
    "text = \"This is a great [MASK].\"\n",
    "preds = predictor(text)\n",
    "for pred in preds:\n",
    "    print(f\">>> {pred['sequence']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we make the model predict a [MASK] in a generic setting, the model seems to veer towards movies as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> i thought the movie was interesting.\n",
      ">>> i thought the film was interesting.\n",
      ">>> i thought the story was interesting.\n",
      ">>> i thought the plot was interesting.\n",
      ">>> i thought the show was interesting.\n"
     ]
    }
   ],
   "source": [
    "text = \"I thought the [MASK] was interesting.\"\n",
    "preds = predictor(text)\n",
    "for pred in preds:\n",
    "    print(f\">>> {pred['sequence']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> a lot of people did not like the film.\n",
      ">>> a lot of people did not like the movie.\n",
      ">>> a lot of people did not like the show.\n",
      ">>> a lot of people did not like the book.\n",
      ">>> a lot of people did not like the story.\n"
     ]
    }
   ],
   "source": [
    "text = \"A lot of people did not like the [MASK].\"\n",
    "preds = predictor(text)\n",
    "for pred in preds:\n",
    "    print(f\">>> {pred['sequence']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e708271ec897467c99848b94c3c63ce8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13767ffdc2d14e398f5bde77fca391e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93217874640646f3b34e9af382cb7374",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.24k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/thuann2cats/distilbert-base-uncased-finetuned-imdb-HF-tutorial-using-trainer/commit/d505f785167ee9e0aa6908a6b9af80cbf02067b5', commit_message='End of training', commit_description='', oid='d505f785167ee9e0aa6908a6b9af80cbf02067b5', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM-finetuning-Coursera",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
